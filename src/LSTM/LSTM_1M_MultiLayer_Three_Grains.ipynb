{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3f351c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58428/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f5709a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_seed = 42\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, save\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a1284a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAA [0, 0, 0, 0, 0, 0, 0]\n",
      "EAG [0, 0, 0, 0, 0, 0, 1]\n",
      "EAC [0, 0, 0, 0, 0, 1, 0]\n",
      "EAT [0, 0, 0, 0, 0, 1, 1]\n",
      "EGA [0, 0, 0, 0, 1, 0, 0]\n",
      "EGG [0, 0, 0, 0, 1, 0, 1]\n",
      "EGC [0, 0, 0, 0, 1, 1, 0]\n",
      "EGT [0, 0, 0, 0, 1, 1, 1]\n",
      "ECA [0, 0, 0, 1, 0, 0, 0]\n",
      "ECG [0, 0, 0, 1, 0, 0, 1]\n",
      "ECC [0, 0, 0, 1, 0, 1, 0]\n",
      "ECT [0, 0, 0, 1, 0, 1, 1]\n",
      "ETA [0, 0, 0, 1, 1, 0, 0]\n",
      "ETG [0, 0, 0, 1, 1, 0, 1]\n",
      "ETC [0, 0, 0, 1, 1, 1, 0]\n",
      "ETT [0, 0, 0, 1, 1, 1, 1]\n",
      "AAE [0, 0, 1, 0, 0, 0, 0]\n",
      "AAA [0, 0, 1, 0, 0, 0, 1]\n",
      "AAG [0, 0, 1, 0, 0, 1, 0]\n",
      "AAC [0, 0, 1, 0, 0, 1, 1]\n",
      "AAT [0, 0, 1, 0, 1, 0, 0]\n",
      "AGE [0, 0, 1, 0, 1, 0, 1]\n",
      "AGA [0, 0, 1, 0, 1, 1, 0]\n",
      "AGG [0, 0, 1, 0, 1, 1, 1]\n",
      "AGC [0, 0, 1, 1, 0, 0, 0]\n",
      "AGT [0, 0, 1, 1, 0, 0, 1]\n",
      "ACE [0, 0, 1, 1, 0, 1, 0]\n",
      "ACA [0, 0, 1, 1, 0, 1, 1]\n",
      "ACG [0, 0, 1, 1, 1, 0, 0]\n",
      "ACC [0, 0, 1, 1, 1, 0, 1]\n",
      "ACT [0, 0, 1, 1, 1, 1, 0]\n",
      "ATE [0, 0, 1, 1, 1, 1, 1]\n",
      "ATA [0, 1, 0, 0, 0, 0, 0]\n",
      "ATG [0, 1, 0, 0, 0, 0, 1]\n",
      "ATC [0, 1, 0, 0, 0, 1, 0]\n",
      "ATT [0, 1, 0, 0, 0, 1, 1]\n",
      "GAE [0, 1, 0, 0, 1, 0, 0]\n",
      "GAA [0, 1, 0, 0, 1, 0, 1]\n",
      "GAG [0, 1, 0, 0, 1, 1, 0]\n",
      "GAC [0, 1, 0, 0, 1, 1, 1]\n",
      "GAT [0, 1, 0, 1, 0, 0, 0]\n",
      "GGE [0, 1, 0, 1, 0, 0, 1]\n",
      "GGA [0, 1, 0, 1, 0, 1, 0]\n",
      "GGG [0, 1, 0, 1, 0, 1, 1]\n",
      "GGC [0, 1, 0, 1, 1, 0, 0]\n",
      "GGT [0, 1, 0, 1, 1, 0, 1]\n",
      "GCE [0, 1, 0, 1, 1, 1, 0]\n",
      "GCA [0, 1, 0, 1, 1, 1, 1]\n",
      "GCG [0, 1, 1, 0, 0, 0, 0]\n",
      "GCC [0, 1, 1, 0, 0, 0, 1]\n",
      "GCT [0, 1, 1, 0, 0, 1, 0]\n",
      "GTE [0, 1, 1, 0, 0, 1, 1]\n",
      "GTA [0, 1, 1, 0, 1, 0, 0]\n",
      "GTG [0, 1, 1, 0, 1, 0, 1]\n",
      "GTC [0, 1, 1, 0, 1, 1, 0]\n",
      "GTT [0, 1, 1, 0, 1, 1, 1]\n",
      "CAE [0, 1, 1, 1, 0, 0, 0]\n",
      "CAA [0, 1, 1, 1, 0, 0, 1]\n",
      "CAG [0, 1, 1, 1, 0, 1, 0]\n",
      "CAC [0, 1, 1, 1, 0, 1, 1]\n",
      "CAT [0, 1, 1, 1, 1, 0, 0]\n",
      "CGE [0, 1, 1, 1, 1, 0, 1]\n",
      "CGA [0, 1, 1, 1, 1, 1, 0]\n",
      "CGG [0, 1, 1, 1, 1, 1, 1]\n",
      "CGC [1, 0, 0, 0, 0, 0, 0]\n",
      "CGT [1, 0, 0, 0, 0, 0, 1]\n",
      "CCE [1, 0, 0, 0, 0, 1, 0]\n",
      "CCA [1, 0, 0, 0, 0, 1, 1]\n",
      "CCG [1, 0, 0, 0, 1, 0, 0]\n",
      "CCC [1, 0, 0, 0, 1, 0, 1]\n",
      "CCT [1, 0, 0, 0, 1, 1, 0]\n",
      "CTE [1, 0, 0, 0, 1, 1, 1]\n",
      "CTA [1, 0, 0, 1, 0, 0, 0]\n",
      "CTG [1, 0, 0, 1, 0, 0, 1]\n",
      "CTC [1, 0, 0, 1, 0, 1, 0]\n",
      "CTT [1, 0, 0, 1, 0, 1, 1]\n",
      "TAE [1, 0, 0, 1, 1, 0, 0]\n",
      "TAA [1, 0, 0, 1, 1, 0, 1]\n",
      "TAG [1, 0, 0, 1, 1, 1, 0]\n",
      "TAC [1, 0, 0, 1, 1, 1, 1]\n",
      "TAT [1, 0, 1, 0, 0, 0, 0]\n",
      "TGE [1, 0, 1, 0, 0, 0, 1]\n",
      "TGA [1, 0, 1, 0, 0, 1, 0]\n",
      "TGG [1, 0, 1, 0, 0, 1, 1]\n",
      "TGC [1, 0, 1, 0, 1, 0, 0]\n",
      "TGT [1, 0, 1, 0, 1, 0, 1]\n",
      "TCE [1, 0, 1, 0, 1, 1, 0]\n",
      "TCA [1, 0, 1, 0, 1, 1, 1]\n",
      "TCG [1, 0, 1, 1, 0, 0, 0]\n",
      "TCC [1, 0, 1, 1, 0, 0, 1]\n",
      "TCT [1, 0, 1, 1, 0, 1, 0]\n",
      "TTE [1, 0, 1, 1, 0, 1, 1]\n",
      "TTA [1, 0, 1, 1, 1, 0, 0]\n",
      "TTG [1, 0, 1, 1, 1, 0, 1]\n",
      "TTC [1, 0, 1, 1, 1, 1, 0]\n",
      "TTT [1, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "DNA_ONE_HOT = {}\n",
    "\n",
    "# E stand for end, which is the head or the tail of the DNA strand\n",
    "ct = 0\n",
    "for pos1 in [\"E\",\"A\",\"G\",\"C\",\"T\"]:\n",
    "    for pos2 in [\"A\",\"G\",\"C\",\"T\"]:\n",
    "        for pos3 in [\"E\",\"A\",\"G\",\"C\",\"T\"]:\n",
    "            comb = pos1+pos2+pos3\n",
    "            if pos1 == \"E\" and pos3 ==\"E\":\n",
    "                continue\n",
    "            elif comb in DNA_ONE_HOT.keys():\n",
    "                continue\n",
    "            else:\n",
    "                DNA_ONE_HOT[comb] = [ int(x) for x in list(f'{ct:07b}') ]  \n",
    "                ct += 1\n",
    "\n",
    "\n",
    "for idx, ele in DNA_ONE_HOT.items():\n",
    "    print(idx,ele)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "833f2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_log = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PADDING = True \n",
    "    \n",
    "MAX_LEN = 60\n",
    "N_FEATURES = 7\n",
    "N_percentage = 0.25 # 1 - 1M\n",
    "\n",
    "DATA_USED_N = 1000000 # max is 1000000\n",
    "# DATA_USED_N = 60 # max is 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c7e8824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data_org)= 1000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strand</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA...</td>\n",
       "      <td>-5.329171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT</td>\n",
       "      <td>-1.687614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG...</td>\n",
       "      <td>-7.224524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG</td>\n",
       "      <td>-7.222121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTCGGGAAGCAAAACCCCTAG</td>\n",
       "      <td>-1.601845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              strand    energy\n",
       "0  GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA... -5.329171\n",
       "1               AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT -1.687614\n",
       "2  CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG... -7.224524\n",
       "3             CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG -7.222121\n",
       "4                              CTCGGGAAGCAAAACCCCTAG -1.601845"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_org = pd.read_csv(\"./data/1mill_dataset.csv\", names=[\"strand\",\"energy\"],dtype={0: str, 1: float})\n",
    "print(\"len(data_org)=\",len(data_org))\n",
    "data_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4d6098d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1000000, train idx 0 - 699999, val idx700000 - 849999, test idx 850000 - 999999\n",
      "\n",
      "\n",
      "len(train_df) : 700000, \n",
      "train_df_all(with reverse) 1400000 \n",
      "len(val_df) : 150000,\n",
      "len(test_df) : 150000\n"
     ]
    }
   ],
   "source": [
    "shuffled_df = data_org.sample(frac = 1, random_state=pandas_seed)\n",
    "shuffled_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# # Clip data, too large can not train in local\n",
    "\n",
    "shuffled_df = shuffled_df.loc[:DATA_USED_N-1]\n",
    "\n",
    "\n",
    "train_pct = 0.7\n",
    "val_pct = 0.15\n",
    "test_pct = 1 - train_pct - val_pct\n",
    "\n",
    "# # shuffled_df.head()\n",
    "all_n = len(shuffled_df)\n",
    "split1 = int(train_pct * all_n) \n",
    "split2 = int((train_pct + val_pct) * all_n) \n",
    "\n",
    "train_df_org = shuffled_df.loc[0:split1-1].copy()\n",
    "val_df = shuffled_df.loc[split1:split2-1].copy()\n",
    "test_df = shuffled_df.loc[split2:,:].copy()\n",
    "\n",
    "train_df_reverse = train_df_org.copy()\n",
    "train_df_reverse[\"strand\"] = train_df_reverse.loc[:,'strand'].apply(lambda x: x[::-1])\n",
    "\n",
    "train_df_all = pd.concat([train_df_org, train_df_reverse])\n",
    "\n",
    "print(f\"total {all_n}, train idx 0 - {split1-1}, val idx{split1} - {split2-1}, test idx {split2} - {all_n-1}\")\n",
    "print(f\"\\n\\nlen(train_df) : {len(train_df_org) }, \\ntrain_df_all(with reverse) {len(train_df_all)} \\nlen(val_df) : {len(val_df)},\\nlen(test_df) : {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df = train_df_all\n",
    "\n",
    "train_val_df = shuffled_df.loc[:split2-1]\n",
    "train_val_mean = train_val_df[\"energy\"].mean()\n",
    "train_val_stdev = train_val_df[\"energy\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b929aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dna_to_threegrain(str1):\n",
    "    str1 = \"E\" + str1 + \"E\"\n",
    "    threegrain = []\n",
    "    length = len(str1)\n",
    "    for idx in range(0,length-2,1):\n",
    "        threegrain.append(str1[idx:idx+3])\n",
    "        \n",
    "    return threegrain\n",
    "\n",
    "# print(convert_dna_to_threegrain(\"AGCTAT\"))\n",
    "\n",
    "\n",
    "def x_transform(dna_str, pad=PADDING):\n",
    "    n_pad = MAX_LEN - len(dna_str)\n",
    "    threegrain_list =  convert_dna_to_threegrain(dna_str)\n",
    "    target = np.array(threegrain_list)\n",
    "    onehot = np.array([DNA_ONE_HOT[tri_letter] for tri_letter in threegrain_list])\n",
    "#     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     print(onehot.shape, fea.shape)\n",
    "    if pad == True:\n",
    "        return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "    else:\n",
    "        return onehot\n",
    "    \n",
    "    \n",
    "def y_transform(energy_float):\n",
    "    y_trans = (energy_float - train_val_mean) / train_val_stdev\n",
    "    return y_trans   \n",
    "\n",
    "def y_transform_reverce(y_array):\n",
    "    y_arr = y_array.cpu().numpy()\n",
    "    y_pred = y_arr * train_val_stdev + train_val_mean\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "948b3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_transform(\"CGTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01e919eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, df, x_transform, y_transform):\n",
    "        self.x_transform = x_transform\n",
    "        self.y_transform = y_transform\n",
    "        self.X = df[\"strand\"].values\n",
    "        self.y = df[\"energy\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        x = self.x_transform(self.X[idx])\n",
    "        y = self.y_transform(self.y[idx])\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# train_dataset = GetData(train_df, x_transform, y_transform)\n",
    "# X,y = train_dataset[1]\n",
    "# print(X.shape)\n",
    "# print(y)\n",
    "\n",
    "train_dataset = TransformDataset(train_df, x_transform, y_transform)\n",
    "val_dataset = TransformDataset(val_df, x_transform, y_transform)\n",
    "test_dataset = TransformDataset(test_df, x_transform, y_transform)\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# X,y = next(iter(train_loader))\n",
    "# print(\"Features shape:\", X.shape)\n",
    "# print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1d285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d342edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_DNA(nn.Module):\n",
    "    def __init__(self, input_dim_n, hidden_units, num_layers,is_bi_dir):\n",
    "        super(LSTM_DNA, self).__init__()\n",
    "        self.input_dim = input_dim_n\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.direction_n = (1 + int(is_bi_dir))\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_dim_n,\n",
    "            hidden_size = hidden_units,\n",
    "            batch_first = True,\n",
    "            num_layers = self.num_layers,\n",
    "            bidirectional = is_bi_dir\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features = self.hidden_units, out_features=45)\n",
    "        self.linear2 = nn.Linear(in_features = 45, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "#         out1 = self.linear1(hn[0]).flatten()\n",
    "        out1 = self.linear1(hn[0])\n",
    "#         print(out1.shape)\n",
    "        out2 = self.linear2(out1).flatten()\n",
    "        \n",
    "        return out2\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         loss = ...\n",
    "#         self.log(\"val_loss\", loss)\n",
    "    \n",
    "# LR = 5e-5\n",
    "# N_HIDDEN_UNITS = 16\n",
    "# model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS)\n",
    "# model = model.float()\n",
    "# loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72647d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00466069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    batch_i = 1\n",
    "    model = model.float()\n",
    "    for X, y in data_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X.float())\n",
    "        loss = loss_function(output, y.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_i += 1\n",
    "#         if batch_i % 200 == 0:\n",
    "#             print(f\"batch {batch_i}, loss = {loss.item()}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches \n",
    "#     print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "def eval_model(data_loader, model, loss_function):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X.float())\n",
    "            total_loss += loss_function(output, y.float()).item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "#     print(f\"   Evalulation loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "\n",
    "def predict(data_loader, model):\n",
    "\n",
    "    output = torch.tensor([])\n",
    "    output = output.to(device)\n",
    "    model.eval()\n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            X = X.to(device)\n",
    "            y_out = model(X.float())\n",
    "            output = torch.cat((output, y_out.float()), 0)\n",
    "    \n",
    "    energy = y_transform_reverce(output)\n",
    "    \n",
    "    return energy\n",
    "\n",
    "def get_avg_mse(y, y_pred):\n",
    "    mse = ((y - y_pred)**2).mean()\n",
    "    print(f\"      current mse: {mse}\")\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a49b75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (150, 2500, 0.01, 90, 1, True)\n",
      "EPOCH150BATCH_SIZE2500LR0.01N_HIDDEN_UNITS90NUM_LAYERS90BI_DIRTrue.pt\n",
      "Epoch 0 finished val_loss0.45869503418604535, Time Elapse:  195.15969626500737\n",
      "Epoch 1 finished val_loss0.4045551603039106, Time Elapse:  193.29888570100593\n",
      "Epoch 2 finished val_loss0.3727587044239044, Time Elapse:  193.40865947799466\n",
      "Epoch 3 finished val_loss0.3486159930626551, Time Elapse:  193.17384038100136\n",
      "Epoch 4 finished val_loss0.3342076157530149, Time Elapse:  192.00984671698825\n",
      "Epoch 5 finished val_loss0.31957096258799234, Time Elapse:  194.69784915199853\n",
      "Epoch 6 finished val_loss0.3107257500290871, Time Elapse:  193.46255999800633\n",
      "Epoch 7 finished val_loss0.30152908662954964, Time Elapse:  192.86920357200142\n",
      "Epoch 8 finished val_loss0.29240326831738156, Time Elapse:  193.53724558799877\n",
      "Epoch 9 finished val_loss0.2867893298467, Time Elapse:  191.1996017870115\n",
      "Epoch 10 finished val_loss0.2811587855219841, Time Elapse:  193.2093236080109\n",
      "Epoch 11 finished val_loss0.27507119129101437, Time Elapse:  192.79212167700462\n",
      "Epoch 12 finished val_loss0.27480832835038504, Time Elapse:  192.4771525010001\n",
      "Epoch 13 finished val_loss0.2689044401049614, Time Elapse:  191.13912141599576\n",
      "Epoch 14 finished val_loss0.26917630260189374, Time Elapse:  191.71048823499586\n",
      "Epoch 15 finished val_loss0.26274035771687826, Time Elapse:  193.2899699349946\n",
      "Epoch 16 finished val_loss0.26026658366123834, Time Elapse:  191.51934218598763\n",
      "Epoch 17 finished val_loss0.2566335342824459, Time Elapse:  191.36410787700152\n",
      "Epoch 18 finished val_loss0.2555396241446336, Time Elapse:  193.1430632200063\n",
      "Epoch 19 finished val_loss0.25273459206024806, Time Elapse:  191.50931975399726\n",
      "Epoch 20 finished val_loss0.25167737205823265, Time Elapse:  191.0852762489958\n",
      "Epoch 21 finished val_loss0.25285678952932356, Time Elapse:  192.64736798200465\n",
      "Epoch 22 finished val_loss0.2490852231780688, Time Elapse:  191.45237865200033\n",
      "Epoch 23 finished val_loss0.24848222757379215, Time Elapse:  193.11008902800677\n",
      "Epoch 24 finished val_loss0.24782734215259553, Time Elapse:  199.38811870099744\n",
      "Epoch 25 finished val_loss0.24685428515076638, Time Elapse:  193.73234219998994\n",
      "Epoch 26 finished val_loss0.2462103473643462, Time Elapse:  192.51240719000634\n",
      "Epoch 27 finished val_loss0.24548324371377628, Time Elapse:  190.58207098100684\n",
      "Epoch 28 finished val_loss0.24586435953776042, Time Elapse:  193.31414577399846\n",
      "Epoch 29 finished val_loss0.2454170987010002, Time Elapse:  193.84356847999152\n",
      "Epoch 30 finished val_loss0.2424789349238078, Time Elapse:  193.52831738701207\n",
      "Epoch 31 finished val_loss0.24898042902350426, Time Elapse:  194.10356723700534\n",
      "Epoch 32 finished val_loss0.24267687574028968, Time Elapse:  190.16014837299008\n",
      "Epoch 33 finished val_loss0.24085616692900658, Time Elapse:  192.61166746899835\n",
      "Epoch 34 finished val_loss0.23897414803504943, Time Elapse:  192.0776045620005\n",
      "Epoch 35 finished val_loss0.24167074933648108, Time Elapse:  194.38153359400167\n",
      "Epoch 36 finished val_loss0.24163926442464193, Time Elapse:  192.56790099800855\n",
      "Epoch 37 finished val_loss0.24268767287333806, Time Elapse:  193.39716848300304\n"
     ]
    }
   ],
   "source": [
    "# for BATCH_SIZE in [1500]:\n",
    "#     for LR in [1e-2]:\n",
    "#         for N_HIDDEN_UNITS in [12]:\n",
    "#             for NUM_LAYERS in [1]:   150\t2500\t0.01\t60\t1\t1000000\t\n",
    "# hyper_paras_list = [\n",
    "#     [150, 2500, 0.01, 90, 1, True]\n",
    "# ]\n",
    "\n",
    "hyper_paras_list = [\n",
    "    [150, 2500, 0.01, 90, 1, True]\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\",\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "\n",
    "for paras in hyper_paras_list:\n",
    "    EPOCH,BATCH_SIZE,LR,N_HIDDEN_UNITS,NUM_LAYERS, BI_DIR = paras\n",
    "    \n",
    "    model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "    model = model.float()\n",
    "    model.to(device)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "    \n",
    "#     train_dataset, val_dataset, test_dataset = train_dataset.to(device), val_dataset.to(device), test_dataset.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    print(f\"EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = {EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, BI_DIR}\")\n",
    "    \n",
    "    min_val_loss = 1e8\n",
    "    lowest_epoch = 0\n",
    "    PATH = f\"EPOCH{EPOCH}BATCH_SIZE{BATCH_SIZE}LR{LR}N_HIDDEN_UNITS{N_HIDDEN_UNITS}NUM_LAYERS{N_HIDDEN_UNITS}BI_DIR{BI_DIR}.pt\"\n",
    "    print(PATH)\n",
    "    \n",
    "    patient = 0\n",
    "    \n",
    "    for ix_epoch in range(EPOCH):\n",
    "#         print(f\"Epoch {ix_epoch} \")\n",
    "        start = timeit.default_timer()\n",
    "        train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "        val_loss = eval_model(val_loader, model, loss_function)\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        if val_loss < min_val_loss:\n",
    "                if os.path.exists(PATH):\n",
    "                    os.remove(PATH)\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "                lowest_epoch = ix_epoch\n",
    "                patient = 0\n",
    "        \n",
    "        else:\n",
    "            patient += 1\n",
    "        \n",
    "        stop = timeit.default_timer()\n",
    "        print(f'Epoch {ix_epoch} finished val_loss{val_loss}, Time Elapse: ', stop - start) \n",
    "        \n",
    "        if patient >= 6:\n",
    "            break\n",
    "        \n",
    "    min_val_loss_id = np.argmin(val_loss_list)\n",
    "    min_val_loss = val_loss_list[min_val_loss_id]\n",
    "    \n",
    "    \n",
    "    model = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    energy_predict = predict(test_loader, model)\n",
    "    test_mse = get_avg_mse(test_df[\"energy\"].values,energy_predict)\n",
    "    \n",
    "    print(f\"Total Epoch {EPOCH}\\n min_val_id {min_val_loss_id}, min_val_loss {min_val_loss}, test_mse {test_mse}\")\n",
    "\n",
    "    model_result = [EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS,BI_DIR, DATA_USED_N,min_val_loss_id, min_val_loss, test_mse, train_loss_list, val_loss_list]\n",
    "\n",
    "    current_df = pd.DataFrame([model_result], columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"BI_DIR\",\"data_used_n\" ,\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "    result_df = pd.concat([result_df,current_df])\n",
    "    letters = string.ascii_lowercase\n",
    "    suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "    print(suffix)\n",
    "    result_df[\"note\"] = \"add another layer at output neural network with 3 grains feature engineering\"\n",
    "    result_df.to_csv(f\"2022-04-29_1_{suffix}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd95728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7445d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2be8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8247c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283fd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240fdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676946f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac296ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc4a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d3330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ddb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd525602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792dcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
