{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3f351c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70767/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5709a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_seed = 42\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, save\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74eb5cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAA [0, 0, 0, 0, 0, 0, 0]\n",
      "EAG [0, 0, 0, 0, 0, 0, 1]\n",
      "EAC [0, 0, 0, 0, 0, 1, 0]\n",
      "EAT [0, 0, 0, 0, 0, 1, 1]\n",
      "EGA [0, 0, 0, 0, 1, 0, 0]\n",
      "EGG [0, 0, 0, 0, 1, 0, 1]\n",
      "EGC [0, 0, 0, 0, 1, 1, 0]\n",
      "EGT [0, 0, 0, 0, 1, 1, 1]\n",
      "ECA [0, 0, 0, 1, 0, 0, 0]\n",
      "ECG [0, 0, 0, 1, 0, 0, 1]\n",
      "ECC [0, 0, 0, 1, 0, 1, 0]\n",
      "ECT [0, 0, 0, 1, 0, 1, 1]\n",
      "ETA [0, 0, 0, 1, 1, 0, 0]\n",
      "ETG [0, 0, 0, 1, 1, 0, 1]\n",
      "ETC [0, 0, 0, 1, 1, 1, 0]\n",
      "ETT [0, 0, 0, 1, 1, 1, 1]\n",
      "AAE [0, 0, 1, 0, 0, 0, 0]\n",
      "AAA [0, 0, 1, 0, 0, 0, 1]\n",
      "AAG [0, 0, 1, 0, 0, 1, 0]\n",
      "AAC [0, 0, 1, 0, 0, 1, 1]\n",
      "AAT [0, 0, 1, 0, 1, 0, 0]\n",
      "AGE [0, 0, 1, 0, 1, 0, 1]\n",
      "AGA [0, 0, 1, 0, 1, 1, 0]\n",
      "AGG [0, 0, 1, 0, 1, 1, 1]\n",
      "AGC [0, 0, 1, 1, 0, 0, 0]\n",
      "AGT [0, 0, 1, 1, 0, 0, 1]\n",
      "ACE [0, 0, 1, 1, 0, 1, 0]\n",
      "ACA [0, 0, 1, 1, 0, 1, 1]\n",
      "ACG [0, 0, 1, 1, 1, 0, 0]\n",
      "ACC [0, 0, 1, 1, 1, 0, 1]\n",
      "ACT [0, 0, 1, 1, 1, 1, 0]\n",
      "ATE [0, 0, 1, 1, 1, 1, 1]\n",
      "ATA [0, 1, 0, 0, 0, 0, 0]\n",
      "ATG [0, 1, 0, 0, 0, 0, 1]\n",
      "ATC [0, 1, 0, 0, 0, 1, 0]\n",
      "ATT [0, 1, 0, 0, 0, 1, 1]\n",
      "GAE [0, 1, 0, 0, 1, 0, 0]\n",
      "GAA [0, 1, 0, 0, 1, 0, 1]\n",
      "GAG [0, 1, 0, 0, 1, 1, 0]\n",
      "GAC [0, 1, 0, 0, 1, 1, 1]\n",
      "GAT [0, 1, 0, 1, 0, 0, 0]\n",
      "GGE [0, 1, 0, 1, 0, 0, 1]\n",
      "GGA [0, 1, 0, 1, 0, 1, 0]\n",
      "GGG [0, 1, 0, 1, 0, 1, 1]\n",
      "GGC [0, 1, 0, 1, 1, 0, 0]\n",
      "GGT [0, 1, 0, 1, 1, 0, 1]\n",
      "GCE [0, 1, 0, 1, 1, 1, 0]\n",
      "GCA [0, 1, 0, 1, 1, 1, 1]\n",
      "GCG [0, 1, 1, 0, 0, 0, 0]\n",
      "GCC [0, 1, 1, 0, 0, 0, 1]\n",
      "GCT [0, 1, 1, 0, 0, 1, 0]\n",
      "GTE [0, 1, 1, 0, 0, 1, 1]\n",
      "GTA [0, 1, 1, 0, 1, 0, 0]\n",
      "GTG [0, 1, 1, 0, 1, 0, 1]\n",
      "GTC [0, 1, 1, 0, 1, 1, 0]\n",
      "GTT [0, 1, 1, 0, 1, 1, 1]\n",
      "CAE [0, 1, 1, 1, 0, 0, 0]\n",
      "CAA [0, 1, 1, 1, 0, 0, 1]\n",
      "CAG [0, 1, 1, 1, 0, 1, 0]\n",
      "CAC [0, 1, 1, 1, 0, 1, 1]\n",
      "CAT [0, 1, 1, 1, 1, 0, 0]\n",
      "CGE [0, 1, 1, 1, 1, 0, 1]\n",
      "CGA [0, 1, 1, 1, 1, 1, 0]\n",
      "CGG [0, 1, 1, 1, 1, 1, 1]\n",
      "CGC [1, 0, 0, 0, 0, 0, 0]\n",
      "CGT [1, 0, 0, 0, 0, 0, 1]\n",
      "CCE [1, 0, 0, 0, 0, 1, 0]\n",
      "CCA [1, 0, 0, 0, 0, 1, 1]\n",
      "CCG [1, 0, 0, 0, 1, 0, 0]\n",
      "CCC [1, 0, 0, 0, 1, 0, 1]\n",
      "CCT [1, 0, 0, 0, 1, 1, 0]\n",
      "CTE [1, 0, 0, 0, 1, 1, 1]\n",
      "CTA [1, 0, 0, 1, 0, 0, 0]\n",
      "CTG [1, 0, 0, 1, 0, 0, 1]\n",
      "CTC [1, 0, 0, 1, 0, 1, 0]\n",
      "CTT [1, 0, 0, 1, 0, 1, 1]\n",
      "TAE [1, 0, 0, 1, 1, 0, 0]\n",
      "TAA [1, 0, 0, 1, 1, 0, 1]\n",
      "TAG [1, 0, 0, 1, 1, 1, 0]\n",
      "TAC [1, 0, 0, 1, 1, 1, 1]\n",
      "TAT [1, 0, 1, 0, 0, 0, 0]\n",
      "TGE [1, 0, 1, 0, 0, 0, 1]\n",
      "TGA [1, 0, 1, 0, 0, 1, 0]\n",
      "TGG [1, 0, 1, 0, 0, 1, 1]\n",
      "TGC [1, 0, 1, 0, 1, 0, 0]\n",
      "TGT [1, 0, 1, 0, 1, 0, 1]\n",
      "TCE [1, 0, 1, 0, 1, 1, 0]\n",
      "TCA [1, 0, 1, 0, 1, 1, 1]\n",
      "TCG [1, 0, 1, 1, 0, 0, 0]\n",
      "TCC [1, 0, 1, 1, 0, 0, 1]\n",
      "TCT [1, 0, 1, 1, 0, 1, 0]\n",
      "TTE [1, 0, 1, 1, 0, 1, 1]\n",
      "TTA [1, 0, 1, 1, 1, 0, 0]\n",
      "TTG [1, 0, 1, 1, 1, 0, 1]\n",
      "TTC [1, 0, 1, 1, 1, 1, 0]\n",
      "TTT [1, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "DNA_ONE_HOT = {}\n",
    "\n",
    "# E stand for end, which is the head or the tail of the DNA strand\n",
    "ct = 0\n",
    "for pos1 in [\"E\",\"A\",\"G\",\"C\",\"T\"]:\n",
    "    for pos2 in [\"A\",\"G\",\"C\",\"T\"]:\n",
    "        for pos3 in [\"E\",\"A\",\"G\",\"C\",\"T\"]:\n",
    "            comb = pos1+pos2+pos3\n",
    "            if pos1 == \"E\" and pos3 ==\"E\":\n",
    "                continue\n",
    "            elif comb in DNA_ONE_HOT.keys():\n",
    "                continue\n",
    "            else:\n",
    "                DNA_ONE_HOT[comb] = [ int(x) for x in list(f'{ct:07b}') ]  \n",
    "                ct += 1\n",
    "\n",
    "\n",
    "for idx, ele in DNA_ONE_HOT.items():\n",
    "    print(idx,ele)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "833f2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_log = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PADDING = True \n",
    "    \n",
    "MAX_LEN = 60\n",
    "N_FEATURES = 7\n",
    "N_percentage = 0.25 # 1 - 1M\n",
    "\n",
    "DATA_USED_N = 1000000 # max is 1000000\n",
    "# DATA_USED_N = 60 # max is 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c7e8824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data_org)= 1000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strand</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA...</td>\n",
       "      <td>-5.329171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT</td>\n",
       "      <td>-1.687614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG...</td>\n",
       "      <td>-7.224524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG</td>\n",
       "      <td>-7.222121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTCGGGAAGCAAAACCCCTAG</td>\n",
       "      <td>-1.601845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              strand    energy\n",
       "0  GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA... -5.329171\n",
       "1               AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT -1.687614\n",
       "2  CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG... -7.224524\n",
       "3             CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG -7.222121\n",
       "4                              CTCGGGAAGCAAAACCCCTAG -1.601845"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_org = pd.read_csv(\"./data/1mill_dataset.csv\", names=[\"strand\",\"energy\"],dtype={0: str, 1: float})\n",
    "print(\"len(data_org)=\",len(data_org))\n",
    "data_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6098d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1000000, train idx 0 - 699999, val idx700000 - 849999, test idx 850000 - 999999\n",
      "\n",
      "\n",
      "len(train_df) : 700000, \n",
      "train_df_all(with reverse) 1400000 \n",
      "len(val_df) : 150000,\n",
      "len(test_df) : 150000\n"
     ]
    }
   ],
   "source": [
    "shuffled_df = data_org.sample(frac = 1, random_state=pandas_seed)\n",
    "shuffled_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# # Clip data, too large can not train in local\n",
    "\n",
    "shuffled_df = shuffled_df.loc[:DATA_USED_N-1]\n",
    "\n",
    "\n",
    "train_pct = 0.7\n",
    "val_pct = 0.15\n",
    "test_pct = 1 - train_pct - val_pct\n",
    "\n",
    "# # shuffled_df.head()\n",
    "all_n = len(shuffled_df)\n",
    "split1 = int(train_pct * all_n) \n",
    "split2 = int((train_pct + val_pct) * all_n) \n",
    "\n",
    "train_df_org = shuffled_df.loc[0:split1-1].copy()\n",
    "val_df = shuffled_df.loc[split1:split2-1].copy()\n",
    "test_df = shuffled_df.loc[split2:,:].copy()\n",
    "\n",
    "train_df_reverse = train_df_org.copy()\n",
    "train_df_reverse[\"strand\"] = train_df_reverse.loc[:,'strand'].apply(lambda x: x[::-1])\n",
    "\n",
    "train_df_all = pd.concat([train_df_org, train_df_reverse])\n",
    "\n",
    "print(f\"total {all_n}, train idx 0 - {split1-1}, val idx{split1} - {split2-1}, test idx {split2} - {all_n-1}\")\n",
    "print(f\"\\n\\nlen(train_df) : {len(train_df_org) }, \\ntrain_df_all(with reverse) {len(train_df_all)} \\nlen(val_df) : {len(val_df)},\\nlen(test_df) : {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df = train_df_all\n",
    "\n",
    "train_val_df = shuffled_df.loc[:split2-1]\n",
    "train_val_mean = train_val_df[\"energy\"].mean()\n",
    "train_val_stdev = train_val_df[\"energy\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34ee059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dna_to_threegrain(str1):\n",
    "    str1 = \"E\" + str1 + \"E\"\n",
    "    threegrain = []\n",
    "    length = len(str1)\n",
    "    for idx in range(0,length-2,1):\n",
    "        threegrain.append(str1[idx:idx+3])\n",
    "        \n",
    "    return threegrain\n",
    "\n",
    "# print(convert_dna_to_threegrain(\"AGCTAT\"))\n",
    "\n",
    "\n",
    "def x_transform(dna_str, pad=PADDING):\n",
    "    n_pad = MAX_LEN - len(dna_str)\n",
    "    threegrain_list =  convert_dna_to_threegrain(dna_str)\n",
    "    target = np.array(threegrain_list)\n",
    "    onehot = np.array([DNA_ONE_HOT[tri_letter] for tri_letter in threegrain_list])\n",
    "#     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     print(onehot.shape, fea.shape)\n",
    "    if pad == True:\n",
    "        return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "    else:\n",
    "        return onehot\n",
    "    \n",
    "    \n",
    "def y_transform(energy_float):\n",
    "    y_trans = (energy_float - train_val_mean) / train_val_stdev\n",
    "    return y_trans   \n",
    "\n",
    "def y_transform_reverce(y_array):\n",
    "    y_arr = y_array.cpu().numpy()\n",
    "    y_pred = y_arr * train_val_stdev + train_val_mean\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "948b3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_transform(\"CGTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01e919eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, df, x_transform, y_transform):\n",
    "        self.x_transform = x_transform\n",
    "        self.y_transform = y_transform\n",
    "        self.X = df[\"strand\"].values\n",
    "        self.y = df[\"energy\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        x = self.x_transform(self.X[idx])\n",
    "        y = self.y_transform(self.y[idx])\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# train_dataset = GetData(train_df, x_transform, y_transform)\n",
    "# X,y = train_dataset[1]\n",
    "# print(X.shape)\n",
    "# print(y)\n",
    "\n",
    "train_dataset = TransformDataset(train_df, x_transform, y_transform)\n",
    "val_dataset = TransformDataset(val_df, x_transform, y_transform)\n",
    "test_dataset = TransformDataset(test_df, x_transform, y_transform)\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# X,y = next(iter(train_loader))\n",
    "# print(\"Features shape:\", X.shape)\n",
    "# print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1d285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d342edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_DNA(nn.Module):\n",
    "    def __init__(self, input_dim_n, hidden_units, num_layers,is_bi_dir):\n",
    "        super(LSTM_DNA, self).__init__()\n",
    "        self.input_dim = input_dim_n\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.direction_n = 1 + int(is_bi_dir)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_dim_n,\n",
    "            hidden_size = hidden_units,\n",
    "            batch_first = True,\n",
    "            num_layers = self.num_layers,\n",
    "            bidirectional = is_bi_dir\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(in_features = self.hidden_units, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten()\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72647d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00466069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    batch_i = 1\n",
    "    model = model.float()\n",
    "    for X, y in data_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X.float())\n",
    "        loss = loss_function(output, y.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_i += 1\n",
    "#         if batch_i % 200 == 0:\n",
    "#             print(f\"batch {batch_i}, loss = {loss.item()}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches \n",
    "#     print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "def eval_model(data_loader, model, loss_function):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X.float())\n",
    "            total_loss += loss_function(output, y.float()).item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "#     print(f\"   Evalulation loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "\n",
    "def predict(data_loader, model):\n",
    "\n",
    "    output = torch.tensor([])\n",
    "    output = output.to(device)\n",
    "    model.eval()\n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            X = X.to(device)\n",
    "            y_out = model(X.float())\n",
    "            output = torch.cat((output, y_out.float()), 0)\n",
    "    \n",
    "    energy = y_transform_reverce(output)\n",
    "    \n",
    "    return energy\n",
    "\n",
    "def get_avg_mse(y, y_pred):\n",
    "    mse = ((y - y_pred)**2).mean()\n",
    "    print(f\"      current mse: {mse}\")\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30a49b75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (150, 2500, 0.01, 90, 1, True)\n",
      "EPOCH150BATCH_SIZE2500LR0.01N_HIDDEN_UNITS90NUM_LAYERS90BI_DIRTrue.pt\n",
      "Epoch 0 finished val_loss 0.4553400814533234, Time Elapse:  197.8842034350091 patient= 0\n",
      "Epoch 1 finished val_loss 0.4097513054807981, Time Elapse:  197.63289113399514 patient= 0\n",
      "Epoch 2 finished val_loss 0.38467051784197487, Time Elapse:  197.61851079599 patient= 0\n",
      "Epoch 3 finished val_loss 0.359900039434433, Time Elapse:  198.38372172199888 patient= 0\n",
      "Epoch 4 finished val_loss 0.33211729377508165, Time Elapse:  196.97354133699264 patient= 0\n",
      "Epoch 5 finished val_loss 0.3156028186281522, Time Elapse:  198.8054471080104 patient= 0\n",
      "Epoch 6 finished val_loss 0.3048651253183683, Time Elapse:  197.9535164609988 patient= 0\n",
      "Epoch 7 finished val_loss 0.2999119753638903, Time Elapse:  197.50871627999004 patient= 0\n",
      "Epoch 8 finished val_loss 0.28767293989658355, Time Elapse:  196.28347878099885 patient= 0\n",
      "Epoch 9 finished val_loss 0.28207789560159047, Time Elapse:  196.89556595799513 patient= 0\n",
      "Epoch 10 finished val_loss 0.27866236567497255, Time Elapse:  196.28913585499686 patient= 0\n",
      "Epoch 11 finished val_loss 0.27809308966000873, Time Elapse:  193.3345250519924 patient= 0\n",
      "Epoch 12 finished val_loss 0.27709065079689027, Time Elapse:  195.31217108099372 patient= 0\n",
      "Epoch 13 finished val_loss 0.27838205695152285, Time Elapse:  197.6999625719909 patient= 1\n",
      "Epoch 14 finished val_loss 0.26460616787274677, Time Elapse:  197.19442136200087 patient= 0\n",
      "Epoch 15 finished val_loss 0.2630369760096073, Time Elapse:  197.04608899400046 patient= 0\n",
      "Epoch 16 finished val_loss 0.2602000226577123, Time Elapse:  196.95005065501027 patient= 0\n",
      "Epoch 17 finished val_loss 0.2588920379678408, Time Elapse:  196.27234696199594 patient= 0\n",
      "Epoch 18 finished val_loss 0.25611567695935566, Time Elapse:  193.8343662249972 patient= 0\n",
      "Epoch 19 finished val_loss 0.26371157690882685, Time Elapse:  196.76755313600006 patient= 1\n",
      "Epoch 20 finished val_loss 0.25562310591340065, Time Elapse:  202.49672266098787 patient= 0\n",
      "Epoch 21 finished val_loss 0.25975160946448644, Time Elapse:  200.79854509800498 patient= 1\n",
      "Epoch 22 finished val_loss 0.25235642393430074, Time Elapse:  200.3401378409908 patient= 0\n",
      "Epoch 23 finished val_loss 0.24957029844323794, Time Elapse:  200.15328643799876 patient= 0\n",
      "Epoch 24 finished val_loss 0.24988385960459708, Time Elapse:  201.1523583829985 patient= 1\n",
      "Epoch 25 finished val_loss 0.25146635894974073, Time Elapse:  201.45934609801043 patient= 2\n",
      "Epoch 26 finished val_loss 0.25108279411991435, Time Elapse:  202.46942244599632 patient= 3\n",
      "Epoch 27 finished val_loss 0.2485433836778005, Time Elapse:  202.2328077699931 patient= 0\n",
      "Epoch 28 finished val_loss 0.2504065444072088, Time Elapse:  202.29519307799637 patient= 1\n",
      "Epoch 29 finished val_loss 0.24552086492379507, Time Elapse:  201.17357513699972 patient= 0\n",
      "Epoch 30 finished val_loss 0.24891205454866092, Time Elapse:  200.6545930860011 patient= 1\n",
      "Epoch 31 finished val_loss 0.24483611385027568, Time Elapse:  201.92733602698718 patient= 0\n",
      "Epoch 32 finished val_loss 0.24696075146396954, Time Elapse:  201.92875312900287 patient= 1\n",
      "Epoch 33 finished val_loss 0.24629355346163115, Time Elapse:  204.3633924559981 patient= 2\n",
      "Epoch 34 finished val_loss 0.2441186601916949, Time Elapse:  202.26828117200057 patient= 0\n",
      "Epoch 35 finished val_loss 0.24978447705507278, Time Elapse:  202.19331584499741 patient= 1\n",
      "Epoch 36 finished val_loss 0.24215595722198485, Time Elapse:  202.3708338770084 patient= 0\n",
      "Epoch 37 finished val_loss 0.24312589342395466, Time Elapse:  202.18784602699452 patient= 1\n",
      "Epoch 38 finished val_loss 0.24387541264295579, Time Elapse:  202.035617704998 patient= 2\n",
      "Epoch 39 finished val_loss 0.2468894732495149, Time Elapse:  202.17709314799868 patient= 3\n",
      "Epoch 40 finished val_loss 0.24125261555115382, Time Elapse:  202.4053616339952 patient= 0\n",
      "Epoch 41 finished val_loss 0.24361019879579543, Time Elapse:  203.50834492700233 patient= 1\n",
      "Epoch 42 finished val_loss 0.24325512424111367, Time Elapse:  201.2686842999974 patient= 2\n",
      "Epoch 43 finished val_loss 0.24228043730060259, Time Elapse:  201.25558209299925 patient= 3\n",
      "Epoch 44 finished val_loss 0.23957833672563236, Time Elapse:  202.51940807800565 patient= 0\n",
      "Epoch 45 finished val_loss 0.2454402767121792, Time Elapse:  200.4092577889969 patient= 1\n",
      "Epoch 46 finished val_loss 0.2402677059173584, Time Elapse:  199.7489343490015 patient= 2\n",
      "Epoch 47 finished val_loss 0.2421962633728981, Time Elapse:  200.09672006900655 patient= 3\n",
      "Epoch 48 finished val_loss 0.24232009500265123, Time Elapse:  199.954586095002 patient= 4\n",
      "Epoch 49 finished val_loss 0.24244429444273313, Time Elapse:  200.46207437600242 patient= 5\n",
      "Epoch 50 finished val_loss 0.23929736763238907, Time Elapse:  199.16807860900008 patient= 0\n",
      "Epoch 51 finished val_loss 0.23877990941206614, Time Elapse:  199.93827345900354 patient= 0\n",
      "Epoch 52 finished val_loss 0.23959859013557433, Time Elapse:  200.50622826600738 patient= 1\n",
      "Epoch 53 finished val_loss 0.23976201886932055, Time Elapse:  199.99909846800438 patient= 2\n",
      "Epoch 54 finished val_loss 0.24064581121007603, Time Elapse:  199.75300951600366 patient= 3\n",
      "Epoch 55 finished val_loss 0.24072262297074, Time Elapse:  200.8838827060099 patient= 4\n",
      "Epoch 56 finished val_loss 0.2388383763531844, Time Elapse:  200.1008661169908 patient= 5\n",
      "Epoch 57 finished val_loss 0.23958115080992382, Time Elapse:  200.51147836000018 patient= 6\n",
      "      current mse: 1.2930766330280545\n",
      "Total Epoch 150\n",
      " min_val_id 51, min_val_loss 0.23877990941206614, test_mse 1.2930766330280545\n",
      "pskunrwrcg\n",
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (150, 2500, 0.01, 120, 1, True)\n",
      "EPOCH150BATCH_SIZE2500LR0.01N_HIDDEN_UNITS120NUM_LAYERS120BI_DIRTrue.pt\n",
      "Epoch 0 finished val_loss 0.43723627279202143, Time Elapse:  217.56941783599905 patient= 0\n",
      "Epoch 1 finished val_loss 0.3971600706378619, Time Elapse:  217.81571527999768 patient= 0\n",
      "Epoch 2 finished val_loss 0.3617384930451711, Time Elapse:  216.92606029700255 patient= 0\n",
      "Epoch 3 finished val_loss 0.3258670464158058, Time Elapse:  218.00808367600257 patient= 0\n",
      "Epoch 4 finished val_loss 0.30504108319679896, Time Elapse:  218.4456638480042 patient= 0\n",
      "Epoch 5 finished val_loss 0.29098825752735136, Time Elapse:  218.31144448499253 patient= 0\n",
      "Epoch 6 finished val_loss 0.28053018301725385, Time Elapse:  217.79675637000764 patient= 0\n",
      "Epoch 7 finished val_loss 0.27118599563837054, Time Elapse:  217.6262060500012 patient= 0\n",
      "Epoch 8 finished val_loss 0.2667405697206656, Time Elapse:  217.9218304570095 patient= 0\n",
      "Epoch 9 finished val_loss 0.2563879465063413, Time Elapse:  217.88768442899163 patient= 0\n",
      "Epoch 10 finished val_loss 0.25236008887489636, Time Elapse:  218.58230191699113 patient= 0\n",
      "Epoch 11 finished val_loss 0.2471824422478676, Time Elapse:  218.2664458000072 patient= 0\n",
      "Epoch 12 finished val_loss 0.2514315980176131, Time Elapse:  218.50290867700824 patient= 1\n",
      "Epoch 13 finished val_loss 0.2419858547548453, Time Elapse:  217.60596338300093 patient= 0\n",
      "Epoch 14 finished val_loss 0.2466406082113584, Time Elapse:  217.36555727200175 patient= 1\n",
      "Epoch 15 finished val_loss 0.23750533858935038, Time Elapse:  217.88378711498808 patient= 0\n",
      "Epoch 16 finished val_loss 0.23506586824854214, Time Elapse:  217.6434222379903 patient= 0\n",
      "Epoch 17 finished val_loss 0.23482719312111536, Time Elapse:  218.0388021550025 patient= 0\n",
      "Epoch 18 finished val_loss 0.23262836635112763, Time Elapse:  217.64501999001368 patient= 0\n",
      "Epoch 19 finished val_loss 0.23470535775025686, Time Elapse:  217.63723002201004 patient= 1\n",
      "Epoch 20 finished val_loss 0.23597870220740635, Time Elapse:  217.42955325500225 patient= 2\n",
      "Epoch 21 finished val_loss 0.23022666722536086, Time Elapse:  217.9895684769872 patient= 0\n",
      "Epoch 22 finished val_loss 0.23100816011428832, Time Elapse:  218.30374742799904 patient= 1\n",
      "Epoch 23 finished val_loss 0.23247693280378978, Time Elapse:  217.23360798700014 patient= 2\n",
      "      current mse: 1.243188204291335\n",
      "Total Epoch 150\n",
      " min_val_id 21, min_val_loss 0.23022666722536086, test_mse 1.243188204291335\n",
      "njinejhbzp\n",
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (150, 2500, 0.01, 90, 2, True)\n",
      "EPOCH150BATCH_SIZE2500LR0.01N_HIDDEN_UNITS90NUM_LAYERS90BI_DIRTrue.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished val_loss 0.4326079105337461, Time Elapse:  258.2091647470079 patient= 0\n",
      "Epoch 1 finished val_loss 0.3941324839989344, Time Elapse:  257.83633826200094 patient= 0\n",
      "Epoch 2 finished val_loss 0.35618937065203987, Time Elapse:  258.1881328980089 patient= 0\n",
      "Epoch 3 finished val_loss 0.32488697171211245, Time Elapse:  258.36884098699375 patient= 0\n",
      "Epoch 4 finished val_loss 0.3106341486175855, Time Elapse:  258.35608437600604 patient= 0\n",
      "Epoch 5 finished val_loss 0.2978302538394928, Time Elapse:  258.489915750004 patient= 0\n",
      "Epoch 6 finished val_loss 0.291835680603981, Time Elapse:  258.06929954899533 patient= 0\n",
      "Epoch 7 finished val_loss 0.28362277199824654, Time Elapse:  259.0732112209953 patient= 0\n",
      "Epoch 8 finished val_loss 0.2755954076846441, Time Elapse:  258.48807166800543 patient= 0\n",
      "Epoch 9 finished val_loss 0.27091822574536006, Time Elapse:  258.1676853819954 patient= 0\n",
      "Epoch 10 finished val_loss 0.27048777987559636, Time Elapse:  258.32732522398874 patient= 0\n",
      "Epoch 11 finished val_loss 0.2625455861290296, Time Elapse:  258.61270091299957 patient= 0\n",
      "Epoch 12 finished val_loss 0.27111558814843495, Time Elapse:  258.1631762350007 patient= 1\n",
      "Epoch 13 finished val_loss 0.2572674296796322, Time Elapse:  258.36389476800105 patient= 0\n",
      "Epoch 14 finished val_loss 0.2560835582514604, Time Elapse:  258.60168245399836 patient= 0\n",
      "Epoch 15 finished val_loss 0.25531135747830075, Time Elapse:  257.28059651199146 patient= 0\n",
      "Epoch 16 finished val_loss 0.25270845666527747, Time Elapse:  258.02023962100793 patient= 0\n",
      "Epoch 17 finished val_loss 0.25057103807727493, Time Elapse:  257.7186196449911 patient= 0\n",
      "Epoch 18 finished val_loss 0.24734842255711556, Time Elapse:  258.03116384500754 patient= 0\n",
      "Epoch 19 finished val_loss 0.24691290979584057, Time Elapse:  257.6924939210003 patient= 0\n",
      "Epoch 20 finished val_loss 0.24688143407305083, Time Elapse:  257.43398032199184 patient= 0\n",
      "Epoch 21 finished val_loss 0.24720294550061225, Time Elapse:  257.99537934899854 patient= 1\n",
      "Epoch 22 finished val_loss 0.2482812911272049, Time Elapse:  257.65103673699195 patient= 2\n",
      "Epoch 23 finished val_loss 0.24370266596476237, Time Elapse:  257.9329331720073 patient= 0\n",
      "Epoch 24 finished val_loss 0.24660351872444153, Time Elapse:  258.2826481180091 patient= 1\n",
      "Epoch 25 finished val_loss 0.24188511272271473, Time Elapse:  257.84924200800015 patient= 0\n",
      "Epoch 26 finished val_loss 0.24121198405822117, Time Elapse:  258.2829228049959 patient= 0\n",
      "Epoch 27 finished val_loss 0.24146007026235264, Time Elapse:  257.6809121520055 patient= 1\n",
      "Epoch 28 finished val_loss 0.2407788227001826, Time Elapse:  258.6724975649995 patient= 0\n",
      "Epoch 29 finished val_loss 0.24402279108762742, Time Elapse:  257.72216556299827 patient= 1\n",
      "Epoch 30 finished val_loss 0.2508815623819828, Time Elapse:  258.03307583699643 patient= 2\n",
      "Epoch 31 finished val_loss 0.2392041342953841, Time Elapse:  258.63292679601 patient= 0\n",
      "Epoch 32 finished val_loss 0.23720552797118824, Time Elapse:  257.8229417029943 patient= 0\n",
      "Epoch 33 finished val_loss 0.23860579878091812, Time Elapse:  258.1574834790081 patient= 1\n",
      "Epoch 34 finished val_loss 0.23776136860251426, Time Elapse:  257.54342548100976 patient= 2\n",
      "Epoch 35 finished val_loss 0.24326786498228709, Time Elapse:  257.7447615860001 patient= 3\n",
      "Epoch 36 finished val_loss 0.23762145414948463, Time Elapse:  257.79745792200265 patient= 4\n",
      "Epoch 37 finished val_loss 0.23763609106341999, Time Elapse:  257.1447938019992 patient= 5\n",
      "Epoch 38 finished val_loss 0.23858096823096275, Time Elapse:  258.30961328200647 patient= 6\n",
      "      current mse: 1.2862361640310978\n",
      "Total Epoch 150\n",
      " min_val_id 32, min_val_loss 0.23720552797118824, test_mse 1.2862361640310978\n",
      "oiulumvzxh\n",
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (150, 2500, 0.08, 100, 1, True)\n",
      "EPOCH150BATCH_SIZE2500LR0.08N_HIDDEN_UNITS100NUM_LAYERS100BI_DIRTrue.pt\n",
      "Epoch 0 finished val_loss 0.673039922118187, Time Elapse:  206.7883671090094 patient= 0\n",
      "Epoch 1 finished val_loss 0.6597417453924815, Time Elapse:  207.34324441799254 patient= 0\n",
      "Epoch 2 finished val_loss 0.6479033917188645, Time Elapse:  206.0441919970035 patient= 0\n",
      "Epoch 3 finished val_loss 0.6105094800392786, Time Elapse:  206.51402253800188 patient= 0\n",
      "Epoch 4 finished val_loss 0.7412623703479767, Time Elapse:  206.8993801829929 patient= 1\n",
      "Epoch 5 finished val_loss 0.5621419052282969, Time Elapse:  207.00106684400816 patient= 0\n",
      "Epoch 6 finished val_loss 0.5419595380624135, Time Elapse:  207.00561430299422 patient= 0\n",
      "Epoch 7 finished val_loss 0.5410242766141892, Time Elapse:  206.8197840780049 patient= 0\n",
      "Epoch 8 finished val_loss 0.642907785375913, Time Elapse:  206.69536646500637 patient= 1\n",
      "Epoch 9 finished val_loss 0.572003252307574, Time Elapse:  206.382592924012 patient= 2\n",
      "Epoch 10 finished val_loss 0.5435661315917969, Time Elapse:  205.63822648799396 patient= 3\n",
      "Epoch 11 finished val_loss 0.49744270741939545, Time Elapse:  206.9674228040094 patient= 0\n",
      "Epoch 12 finished val_loss 0.48404094129800795, Time Elapse:  206.27976182699786 patient= 0\n",
      "Epoch 13 finished val_loss 0.4793137113253276, Time Elapse:  206.1684448619926 patient= 0\n",
      "Epoch 14 finished val_loss 0.4718880986173948, Time Elapse:  206.7312081949931 patient= 0\n",
      "Epoch 15 finished val_loss 0.47527424345413843, Time Elapse:  206.38445200200658 patient= 1\n",
      "Epoch 16 finished val_loss 0.46796255856752395, Time Elapse:  207.95342710000114 patient= 0\n",
      "Epoch 17 finished val_loss 0.45946353922287625, Time Elapse:  207.62007047000225 patient= 0\n",
      "Epoch 18 finished val_loss 0.5151601925492286, Time Elapse:  206.6859615620051 patient= 1\n",
      "Epoch 19 finished val_loss 0.4463791499535243, Time Elapse:  206.3379470439977 patient= 0\n",
      "Epoch 20 finished val_loss 0.46902458419402443, Time Elapse:  206.4042557280045 patient= 1\n",
      "Epoch 21 finished val_loss 0.44696855743726094, Time Elapse:  205.6833852790005 patient= 2\n",
      "Epoch 22 finished val_loss 0.4302647386988004, Time Elapse:  206.29750687800697 patient= 0\n",
      "Epoch 23 finished val_loss 0.4401389574011167, Time Elapse:  206.1557216049987 patient= 1\n",
      "Epoch 24 finished val_loss 0.42386996348698935, Time Elapse:  206.65890854399186 patient= 0\n",
      "Epoch 25 finished val_loss 0.4229478493332863, Time Elapse:  206.4602603710082 patient= 0\n",
      "Epoch 26 finished val_loss 0.41424040247996646, Time Elapse:  206.71986843299237 patient= 0\n",
      "Epoch 27 finished val_loss 0.4038702080647151, Time Elapse:  206.8403523839952 patient= 0\n",
      "Epoch 28 finished val_loss 0.40564166754484177, Time Elapse:  206.7731502210081 patient= 1\n",
      "Epoch 29 finished val_loss 0.39860572963953017, Time Elapse:  206.51538971200353 patient= 0\n",
      "Epoch 30 finished val_loss 0.3965331902106603, Time Elapse:  207.01041425898438 patient= 0\n",
      "Epoch 31 finished val_loss 0.3808397963643074, Time Elapse:  205.7626784000022 patient= 0\n",
      "Epoch 32 finished val_loss 0.3794676706194878, Time Elapse:  205.46098211797653 patient= 0\n",
      "Epoch 33 finished val_loss 0.37597569276889165, Time Elapse:  208.48094347299775 patient= 0\n",
      "Epoch 34 finished val_loss 0.3744863525032997, Time Elapse:  209.49483244298608 patient= 0\n",
      "Epoch 35 finished val_loss 0.38940056016047797, Time Elapse:  207.48480076700798 patient= 1\n",
      "Epoch 36 finished val_loss 0.3572387342651685, Time Elapse:  207.48947592600598 patient= 0\n",
      "Epoch 37 finished val_loss 0.3578235412637393, Time Elapse:  207.95748745798483 patient= 1\n",
      "Epoch 38 finished val_loss 0.35297327836354575, Time Elapse:  207.83317825899576 patient= 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ix_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCH):\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#         print(f\"Epoch {ix_epoch} \")\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         start \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[0;32m---> 50\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m eval_model(val_loader, model, loss_function)\n\u001b[1;32m     53\u001b[0m         train_loss_list\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data_loader, model, loss_function, optimizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m      9\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Downloads/venvpy39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Downloads/venvpy39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/Downloads/venvpy39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/venvpy39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mTransformDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx): \n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_transform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[idx])\n\u001b[0;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(x), torch\u001b[38;5;241m.\u001b[39mtensor(y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for BATCH_SIZE in [1500]:\n",
    "#     for LR in [1e-2]:\n",
    "#         for N_HIDDEN_UNITS in [12]:\n",
    "#             for NUM_LAYERS in [1]:   150\t2500\t0.01\t60\t1\t1000000\t\n",
    "# hyper_paras_list = [\n",
    "#     [150, 2500, 0.01, 90, 1, True]\n",
    "# ]\n",
    "\n",
    "hyper_paras_list = [\n",
    "    [150, 2500, 0.01, 90, 1, True],\n",
    "    [150, 2500, 0.01, 120, 1, True],\n",
    "    [150, 2500, 0.01, 90, 2, True],\n",
    "    [150, 2500, 0.08, 100, 1, True]\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\",\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "\n",
    "for paras in hyper_paras_list:\n",
    "    EPOCH,BATCH_SIZE,LR,N_HIDDEN_UNITS,NUM_LAYERS, BI_DIR = paras\n",
    "    \n",
    "    model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "    model = model.float()\n",
    "    model.to(device)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "    \n",
    "#     train_dataset, val_dataset, test_dataset = train_dataset.to(device), val_dataset.to(device), test_dataset.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    print(f\"EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = {EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, BI_DIR}\")\n",
    "    \n",
    "    min_val_loss = 1e8\n",
    "    lowest_epoch = 0\n",
    "    PATH = f\"EPOCH{EPOCH}BATCH_SIZE{BATCH_SIZE}LR{LR}N_HIDDEN_UNITS{N_HIDDEN_UNITS}NUM_LAYERS{NUM_LAYERS}BI_DIR{BI_DIR}.pt\"\n",
    "    print(PATH)\n",
    "    \n",
    "    patient = 0\n",
    "    \n",
    "    for ix_epoch in range(EPOCH):\n",
    "#         print(f\"Epoch {ix_epoch} \")\n",
    "        start = timeit.default_timer()\n",
    "        train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "        val_loss = eval_model(val_loader, model, loss_function)\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        if val_loss < min_val_loss:\n",
    "                if os.path.exists(PATH):\n",
    "                    os.remove(PATH)\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "                lowest_epoch = ix_epoch\n",
    "                patient = 0\n",
    "                \n",
    "        elif val_loss > min_val_loss * 2:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            patient += 1\n",
    "            \n",
    "        \n",
    "        stop = timeit.default_timer()\n",
    "        print(f'Epoch {ix_epoch} finished val_loss {val_loss}, Time Elapse: ', stop - start, \"patient=\", patient) \n",
    "        \n",
    "        if patient >= 6:\n",
    "            break\n",
    "        \n",
    "    min_val_loss_id = np.argmin(val_loss_list)\n",
    "    min_val_loss = val_loss_list[min_val_loss_id]\n",
    "    \n",
    "    \n",
    "    model = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    energy_predict = predict(test_loader, model)\n",
    "    test_mse = get_avg_mse(test_df[\"energy\"].values,energy_predict)\n",
    "    \n",
    "    print(f\"Total Epoch {EPOCH}\\n min_val_id {min_val_loss_id}, min_val_loss {min_val_loss}, test_mse {test_mse}\")\n",
    "\n",
    "    model_result = [EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS,BI_DIR, DATA_USED_N,min_val_loss_id, min_val_loss, test_mse, train_loss_list, val_loss_list]\n",
    "\n",
    "    current_df = pd.DataFrame([model_result], columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"BI_DIR\",\"data_used_n\" ,\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "    result_df = pd.concat([result_df,current_df])\n",
    "    letters = string.ascii_lowercase\n",
    "    suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "    print(suffix)\n",
    "    result_df[\"note\"] = \"only one layer for output neural network with 3 grains feature engineering\"\n",
    "    result_df.to_csv(f\"2022-04-29_2_{suffix}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd95728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# hyper_paras_list = [\n",
    "#     [150, 2500, 0.01, 120, 1, True],\n",
    "#     [150, 2000, 0.01, 120, 2, True],\n",
    "#     [150, 2500, 0.01, 140, 2, True]\n",
    "# ]\n",
    "\n",
    "# result_df = pd.DataFrame(columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\",\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "\n",
    "# for paras in hyper_paras_list:\n",
    "#     EPOCH,BATCH_SIZE,LR,N_HIDDEN_UNITS,NUM_LAYERS, BI_DIR = paras\n",
    "    \n",
    "#     model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "#     model = model.float()\n",
    "#     model.to(device)\n",
    "#     loss_function = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "    \n",
    "# #     train_dataset, val_dataset, test_dataset = train_dataset.to(device), val_dataset.to(device), test_dataset.to(device)\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     model_list = []\n",
    "#     train_loss_list = []\n",
    "#     val_loss_list = []\n",
    "\n",
    "#     print(f\"EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = {EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, BI_DIR}\")\n",
    "    \n",
    "#     min_val_loss = 1e8\n",
    "#     lowest_epoch = 0\n",
    "#     PATH = f\"EPOCH{EPOCH}BATCH_SIZE{BATCH_SIZE}LR{LR}N_HIDDEN_UNITS{N_HIDDEN_UNITS}NUM_LAYERS{NUM_LAYERS}BI_DIR{BI_DIR}.pt\"\n",
    "#     print(PATH)\n",
    "    \n",
    "#     patient = 0\n",
    "    \n",
    "#     for ix_epoch in range(EPOCH):\n",
    "# #         print(f\"Epoch {ix_epoch} \")\n",
    "#         start = timeit.default_timer()\n",
    "#         train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "#         val_loss = eval_model(val_loader, model, loss_function)\n",
    "        \n",
    "#         train_loss_list.append(train_loss)\n",
    "#         val_loss_list.append(val_loss)\n",
    "        \n",
    "#         if val_loss < min_val_loss:\n",
    "#                 if os.path.exists(PATH):\n",
    "#                     os.remove(PATH)\n",
    "#                 torch.save(model.state_dict(), PATH)\n",
    "#                 min_val_loss = val_loss\n",
    "#                 lowest_epoch = ix_epoch\n",
    "#                 patient = 0\n",
    "                \n",
    "#         elif val_loss > min_val_loss * 2:\n",
    "#             break\n",
    "            \n",
    "#         else:\n",
    "#             patient += 1\n",
    "            \n",
    "        \n",
    "#         stop = timeit.default_timer()\n",
    "#         print(f'Epoch {ix_epoch} finished val_loss {val_loss}, Time Elapse: ', stop - start, \"patient=\", patient) \n",
    "        \n",
    "#         if patient >= 6:\n",
    "#             break\n",
    "        \n",
    "#     min_val_loss_id = np.argmin(val_loss_list)\n",
    "#     min_val_loss = val_loss_list[min_val_loss_id]\n",
    "    \n",
    "    \n",
    "#     model = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "#     model.load_state_dict(torch.load(PATH))\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     energy_predict = predict(test_loader, model)\n",
    "#     test_mse = get_avg_mse(test_df[\"energy\"].values,energy_predict)\n",
    "    \n",
    "#     print(f\"Total Epoch {EPOCH}\\n min_val_id {min_val_loss_id}, min_val_loss {min_val_loss}, test_mse {test_mse}\")\n",
    "\n",
    "#     model_result = [EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS,BI_DIR, DATA_USED_N,min_val_loss_id, min_val_loss, test_mse, train_loss_list, val_loss_list]\n",
    "\n",
    "#     current_df = pd.DataFrame([model_result], columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"BI_DIR\",\"data_used_n\" ,\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "#     result_df = pd.concat([result_df,current_df])\n",
    "#     letters = string.ascii_lowercase\n",
    "#     suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "#     print(suffix)\n",
    "#     result_df[\"note\"] = \"only one layer for output neural network with 3 grains feature engineering\"\n",
    "#     result_df.to_csv(f\"2022-04-30_3_{suffix}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7445d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2be8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283fd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240fdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676946f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac296ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc4a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d3330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ddb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd525602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792dcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
