{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3f351c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114190/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5709a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_seed = 42\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, save\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74eb5cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E stand for end, which is the head or the tail of the DNA strand\n",
      "24\n",
      "EA [0, 0, 0, 0, 0]\n",
      "EG [0, 0, 0, 0, 1]\n",
      "EC [0, 0, 0, 1, 0]\n",
      "ET [0, 0, 0, 1, 1]\n",
      "AE [0, 0, 1, 0, 0]\n",
      "AA [0, 0, 1, 0, 1]\n",
      "AG [0, 0, 1, 1, 0]\n",
      "AC [0, 0, 1, 1, 1]\n",
      "AT [0, 1, 0, 0, 0]\n",
      "GE [0, 1, 0, 0, 1]\n",
      "GA [0, 1, 0, 1, 0]\n",
      "GG [0, 1, 0, 1, 1]\n",
      "GC [0, 1, 1, 0, 0]\n",
      "GT [0, 1, 1, 0, 1]\n",
      "CE [0, 1, 1, 1, 0]\n",
      "CA [0, 1, 1, 1, 1]\n",
      "CG [1, 0, 0, 0, 0]\n",
      "CC [1, 0, 0, 0, 1]\n",
      "CT [1, 0, 0, 1, 0]\n",
      "TE [1, 0, 0, 1, 1]\n",
      "TA [1, 0, 1, 0, 0]\n",
      "TG [1, 0, 1, 0, 1]\n",
      "TC [1, 0, 1, 1, 0]\n",
      "TT [1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "DNA_ONE_HOT = {}\n",
    "\n",
    "# E stand for end, which is the head or the tail of the DNA strand\n",
    "ct = 0\n",
    "for pos1 in [\"E\",\"A\",\"G\",\"C\",\"T\"]:\n",
    "    for pos2 in [\"E\",\"A\",\"G\",\"C\",\"T\"]:\n",
    "            comb = pos1+pos2\n",
    "            if pos1 == \"E\" and pos2 ==\"E\":\n",
    "                continue\n",
    "            elif comb in DNA_ONE_HOT.keys():\n",
    "                continue\n",
    "            else:\n",
    "                DNA_ONE_HOT[comb] = [ int(x) for x in list(f'{ct:05b}') ]  \n",
    "                ct += 1\n",
    "print(\"E stand for end, which is the head or the tail of the DNA strand\")\n",
    "print(len(DNA_ONE_HOT))\n",
    "for idx, ele in DNA_ONE_HOT.items():\n",
    "    print(idx,ele)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "833f2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_log = []\n",
    "\n",
    "\n",
    "PADDING = True \n",
    "    \n",
    "MAX_LEN = 60\n",
    "N_FEATURES = 5\n",
    "N_percentage = 0.25 # 1 - 1M\n",
    "\n",
    "DATA_USED_N = 1000000 # max is 1000000\n",
    "# DATA_USED_N = 60 # max is 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c7e8824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data_org)= 1000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strand</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA...</td>\n",
       "      <td>-5.329171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT</td>\n",
       "      <td>-1.687614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG...</td>\n",
       "      <td>-7.224524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG</td>\n",
       "      <td>-7.222121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTCGGGAAGCAAAACCCCTAG</td>\n",
       "      <td>-1.601845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              strand    energy\n",
       "0  GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA... -5.329171\n",
       "1               AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT -1.687614\n",
       "2  CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG... -7.224524\n",
       "3             CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG -7.222121\n",
       "4                              CTCGGGAAGCAAAACCCCTAG -1.601845"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_org = pd.read_csv(\"./data/1mill_dataset.csv\", names=[\"strand\",\"energy\"],dtype={0: str, 1: float})\n",
    "print(\"len(data_org)=\",len(data_org))\n",
    "data_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6098d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1000000, train idx 0 - 699999, val idx700000 - 849999, test idx 850000 - 999999\n",
      "\n",
      "\n",
      "len(train_df) : 700000, \n",
      "train_df_all(with reverse) 1400000 \n",
      "len(val_df) : 150000,\n",
      "len(test_df) : 150000\n"
     ]
    }
   ],
   "source": [
    "shuffled_df = data_org.sample(frac = 1, random_state=pandas_seed)\n",
    "shuffled_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# # Clip data, too large can not train in local\n",
    "\n",
    "shuffled_df = shuffled_df.loc[:DATA_USED_N-1]\n",
    "\n",
    "\n",
    "train_pct = 0.7\n",
    "val_pct = 0.15\n",
    "test_pct = 1 - train_pct - val_pct\n",
    "\n",
    "# # shuffled_df.head()\n",
    "all_n = len(shuffled_df)\n",
    "split1 = int(train_pct * all_n) \n",
    "split2 = int((train_pct + val_pct) * all_n) \n",
    "\n",
    "train_df_org = shuffled_df.loc[0:split1-1].copy()\n",
    "val_df = shuffled_df.loc[split1:split2-1].copy()\n",
    "test_df = shuffled_df.loc[split2:,:].copy()\n",
    "\n",
    "train_df_reverse = train_df_org.copy()\n",
    "train_df_reverse[\"strand\"] = train_df_reverse.loc[:,'strand'].apply(lambda x: x[::-1])\n",
    "\n",
    "train_df_all = pd.concat([train_df_org, train_df_reverse])\n",
    "\n",
    "print(f\"total {all_n}, train idx 0 - {split1-1}, val idx{split1} - {split2-1}, test idx {split2} - {all_n-1}\")\n",
    "print(f\"\\n\\nlen(train_df) : {len(train_df_org) }, \\ntrain_df_all(with reverse) {len(train_df_all)} \\nlen(val_df) : {len(val_df)},\\nlen(test_df) : {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df = train_df_all\n",
    "\n",
    "train_val_df = shuffled_df.loc[:split2-1]\n",
    "train_val_mean = train_val_df[\"energy\"].mean()\n",
    "train_val_stdev = train_val_df[\"energy\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34ee059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EA', 'AG', 'GC', 'CT', 'TA', 'AT', 'TE']\n"
     ]
    }
   ],
   "source": [
    "def convert_dna_to_threegrain(str1):\n",
    "    str1 = \"E\" + str1 + \"E\"\n",
    "    threegrain = []\n",
    "    length = len(str1)\n",
    "    for idx in range(0,length-1,1):\n",
    "        threegrain.append(str1[idx:idx+2])\n",
    "        \n",
    "    return threegrain\n",
    "\n",
    "print(convert_dna_to_threegrain(\"AGCTAT\"))\n",
    "\n",
    "\n",
    "def x_transform(dna_str, pad=PADDING):\n",
    "    n_pad = MAX_LEN - len(dna_str)\n",
    "    threegrain_list =  convert_dna_to_threegrain(dna_str)\n",
    "    target = np.array(threegrain_list)\n",
    "    onehot = np.array([DNA_ONE_HOT[tri_letter] for tri_letter in threegrain_list])\n",
    "#     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     print(onehot.shape, fea.shape)\n",
    "    if pad == True:\n",
    "        return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "    else:\n",
    "        return onehot\n",
    "    \n",
    "    \n",
    "def y_transform(energy_float):\n",
    "    y_trans = (energy_float - train_val_mean) / train_val_stdev\n",
    "    return y_trans   \n",
    "\n",
    "def y_transform_reverce(y_array):\n",
    "    y_arr = y_array.cpu().numpy()\n",
    "    y_pred = y_arr * train_val_stdev + train_val_mean\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "948b3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_transform(\"CGTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01e919eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, df, x_transform, y_transform):\n",
    "        self.x_transform = x_transform\n",
    "        self.y_transform = y_transform\n",
    "        self.X = df[\"strand\"].values\n",
    "        self.y = df[\"energy\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        x = self.x_transform(self.X[idx])\n",
    "        y = self.y_transform(self.y[idx])\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# train_dataset = GetData(train_df, x_transform, y_transform)\n",
    "# X,y = train_dataset[1]\n",
    "# print(X.shape)\n",
    "# print(y)\n",
    "\n",
    "train_dataset = TransformDataset(train_df, x_transform, y_transform)\n",
    "val_dataset = TransformDataset(val_df, x_transform, y_transform)\n",
    "test_dataset = TransformDataset(test_df, x_transform, y_transform)\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# X,y = next(iter(train_loader))\n",
    "# print(\"Features shape:\", X.shape)\n",
    "# print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1d285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d342edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_DNA(nn.Module):\n",
    "    def __init__(self, input_dim_n, hidden_units, num_layers,is_bi_dir):\n",
    "        super(LSTM_DNA, self).__init__()\n",
    "        self.input_dim = input_dim_n\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.direction_n = (1 + int(is_bi_dir))\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_dim_n,\n",
    "            hidden_size = hidden_units,\n",
    "            batch_first = True,\n",
    "            num_layers = self.num_layers,\n",
    "            bidirectional = is_bi_dir\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features = self.hidden_units, out_features=45)\n",
    "        self.linear2 = nn.Linear(in_features = 45, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "#         out1 = self.linear1(hn[0]).flatten()\n",
    "        out1 = self.linear1(hn[0])\n",
    "#         print(out1.shape)\n",
    "        out2 = self.linear2(out1).flatten()\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72647d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00466069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    batch_i = 1\n",
    "    model = model.float()\n",
    "    for X, y in data_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X.float())\n",
    "        loss = loss_function(output, y.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_i += 1\n",
    "#         if batch_i % 200 == 0:\n",
    "#             print(f\"batch {batch_i}, loss = {loss.item()}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches \n",
    "#     print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "def eval_model(data_loader, model, loss_function):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X.float())\n",
    "            total_loss += loss_function(output, y.float()).item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "#     print(f\"   Evalulation loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "\n",
    "def predict(data_loader, model):\n",
    "\n",
    "    output = torch.tensor([])\n",
    "    output = output.to(device)\n",
    "    model.eval()\n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            X = X.to(device)\n",
    "            y_out = model(X.float())\n",
    "            output = torch.cat((output, y_out.float()), 0)\n",
    "    \n",
    "    energy = y_transform_reverce(output)\n",
    "    \n",
    "    return energy\n",
    "\n",
    "def get_avg_mse(y, y_pred):\n",
    "    mse = ((y - y_pred)**2).mean()\n",
    "    print(f\"      current mse: {mse}\")\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a49b75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (150, 1500, 0.01, 90, 2, True)\n",
      "EPOCH150BATCH_SIZE1500LR0.01N_HIDDEN_UNITS90NUM_LAYERS2BI_DIRTrue.pt\n",
      "Epoch 0 finished val_loss 0.41496407449245454, Time Elapse:  247.05748301299172 patient= 0\n",
      "Epoch 1 finished val_loss 0.36412972897291185, Time Elapse:  247.86586745700333 patient= 0\n",
      "Epoch 2 finished val_loss 0.3284513261914253, Time Elapse:  247.4604302099906 patient= 0\n",
      "Epoch 3 finished val_loss 0.31161707639694214, Time Elapse:  247.6984825609834 patient= 0\n",
      "Epoch 4 finished val_loss 0.2943620117008686, Time Elapse:  246.82571674499195 patient= 0\n",
      "Epoch 5 finished val_loss 0.28727971464395524, Time Elapse:  247.8459660759836 patient= 0\n",
      "Epoch 6 finished val_loss 0.2767387245595455, Time Elapse:  248.02467365699704 patient= 0\n",
      "Epoch 7 finished val_loss 0.2713864703476429, Time Elapse:  247.95520609398955 patient= 0\n",
      "Epoch 8 finished val_loss 0.2682210622727871, Time Elapse:  248.16135141800623 patient= 0\n",
      "Epoch 9 finished val_loss 0.2617457848787308, Time Elapse:  247.74581034100265 patient= 0\n",
      "Epoch 10 finished val_loss 0.2705640210211277, Time Elapse:  248.04124186298577 patient= 1\n",
      "Epoch 11 finished val_loss 0.2555475364625454, Time Elapse:  248.10482790801325 patient= 0\n",
      "Epoch 12 finished val_loss 0.25113709524273875, Time Elapse:  247.2328093769902 patient= 0\n",
      "Epoch 13 finished val_loss 0.25229412600398066, Time Elapse:  248.24412727201707 patient= 1\n",
      "Epoch 14 finished val_loss 0.24582121059298515, Time Elapse:  247.16800079701352 patient= 0\n",
      "Epoch 15 finished val_loss 0.24398066833615303, Time Elapse:  247.8877340409963 patient= 0\n",
      "Epoch 16 finished val_loss 0.24286525189876557, Time Elapse:  247.56358297599945 patient= 0\n",
      "Epoch 17 finished val_loss 0.2432222330570221, Time Elapse:  248.54209829002502 patient= 1\n",
      "Epoch 18 finished val_loss 0.24195133790373802, Time Elapse:  247.9747102120018 patient= 0\n",
      "Epoch 19 finished val_loss 0.25932534500956533, Time Elapse:  247.84752376799588 patient= 1\n",
      "Epoch 20 finished val_loss 0.2421676044166088, Time Elapse:  248.21026863000588 patient= 2\n",
      "Epoch 21 finished val_loss 0.24062130406498908, Time Elapse:  247.66499682399444 patient= 0\n",
      "Epoch 22 finished val_loss 0.2510770633816719, Time Elapse:  248.74006756802555 patient= 1\n",
      "Epoch 23 finished val_loss 0.238747688382864, Time Elapse:  247.87746023698128 patient= 0\n",
      "Epoch 24 finished val_loss 0.25728138238191606, Time Elapse:  247.52767723900615 patient= 1\n",
      "Epoch 25 finished val_loss 0.23492819249629973, Time Elapse:  247.55947049800307 patient= 0\n",
      "Epoch 26 finished val_loss 0.23346127867698668, Time Elapse:  248.23865707599907 patient= 0\n",
      "Epoch 27 finished val_loss 0.23438595443964005, Time Elapse:  247.37434316100553 patient= 1\n",
      "Epoch 28 finished val_loss 0.23352758437395096, Time Elapse:  247.24500096499105 patient= 2\n",
      "Epoch 29 finished val_loss 0.23232886478304862, Time Elapse:  247.8600891349779 patient= 0\n",
      "Epoch 30 finished val_loss 0.2319750303030014, Time Elapse:  247.48233918100595 patient= 0\n",
      "Epoch 31 finished val_loss 0.23034053519368172, Time Elapse:  248.1465535799798 patient= 0\n",
      "Epoch 32 finished val_loss 0.23246322989463805, Time Elapse:  247.5657131759799 patient= 1\n",
      "Epoch 33 finished val_loss 0.23461711496114732, Time Elapse:  248.039961803006 patient= 2\n",
      "Epoch 34 finished val_loss 0.23167467683553697, Time Elapse:  247.8516856310016 patient= 3\n",
      "Epoch 35 finished val_loss 0.2326886385679245, Time Elapse:  247.21600117097842 patient= 4\n",
      "Epoch 36 finished val_loss 0.22914128452539445, Time Elapse:  247.68154013701132 patient= 0\n",
      "Epoch 37 finished val_loss 0.23033906221389772, Time Elapse:  248.45415530598257 patient= 1\n",
      "Epoch 38 finished val_loss 0.23062764942646027, Time Elapse:  248.32178745200508 patient= 2\n",
      "Epoch 39 finished val_loss 0.22894657343626024, Time Elapse:  248.22192457699566 patient= 0\n",
      "Epoch 40 finished val_loss 0.2284299398958683, Time Elapse:  248.2078119929938 patient= 0\n",
      "Epoch 41 finished val_loss 0.22756058290600778, Time Elapse:  247.64559538001777 patient= 0\n",
      "Epoch 42 finished val_loss 0.23221140533685683, Time Elapse:  248.54182780897827 patient= 1\n",
      "Epoch 43 finished val_loss 0.22926013678312301, Time Elapse:  247.87823813600698 patient= 2\n",
      "Epoch 44 finished val_loss 0.22895008325576782, Time Elapse:  248.18825846799882 patient= 3\n",
      "Epoch 45 finished val_loss 0.23086969837546348, Time Elapse:  247.91891289502382 patient= 4\n",
      "Epoch 46 finished val_loss 0.22753555551171303, Time Elapse:  247.57088649799698 patient= 0\n",
      "Epoch 47 finished val_loss 0.22976078525185584, Time Elapse:  247.31232917198213 patient= 1\n",
      "Epoch 48 finished val_loss 0.22680252611637117, Time Elapse:  247.70443299799808 patient= 0\n",
      "Epoch 49 finished val_loss 0.22826083213090897, Time Elapse:  247.6161465460027 patient= 1\n",
      "Epoch 50 finished val_loss 0.22763989746570587, Time Elapse:  247.49518094799714 patient= 2\n",
      "Epoch 51 finished val_loss 0.22721117615699768, Time Elapse:  248.61812195699895 patient= 3\n",
      "Epoch 52 finished val_loss 0.22597703695297242, Time Elapse:  248.48899320000783 patient= 0\n",
      "Epoch 53 finished val_loss 0.2273953965306282, Time Elapse:  248.27514196699485 patient= 1\n",
      "Epoch 54 finished val_loss 0.22817201256752015, Time Elapse:  248.2905205390125 patient= 2\n",
      "Epoch 55 finished val_loss 0.22733913540840148, Time Elapse:  248.4971209600044 patient= 3\n",
      "Epoch 56 finished val_loss 0.23245173186063767, Time Elapse:  248.40011937299278 patient= 4\n",
      "Epoch 57 finished val_loss 0.22892489954829215, Time Elapse:  247.82728595900699 patient= 5\n",
      "Epoch 58 finished val_loss 0.22653605476021765, Time Elapse:  248.56593253000756 patient= 6\n",
      "      current mse: 1.2173244810428572\n",
      "Total Epoch 150\n",
      " min_val_id 52, min_val_loss 0.22597703695297242, test_mse 1.2173244810428572\n",
      "xvbwzayiqq\n",
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (150, 1500, 0.01, 120, 2, True)\n",
      "EPOCH150BATCH_SIZE1500LR0.01N_HIDDEN_UNITS120NUM_LAYERS2BI_DIRTrue.pt\n",
      "Epoch 0 finished val_loss 0.42006400048732756, Time Elapse:  298.60848322100355 patient= 0\n",
      "Epoch 1 finished val_loss 0.36867033064365384, Time Elapse:  298.72658739000326 patient= 0\n",
      "Epoch 2 finished val_loss 0.3251728218793869, Time Elapse:  299.1451706739899 patient= 0\n",
      "Epoch 3 finished val_loss 0.3079896891117096, Time Elapse:  299.80494637897937 patient= 0\n",
      "Epoch 4 finished val_loss 0.28404165744781495, Time Elapse:  299.1901318820019 patient= 0\n",
      "Epoch 5 finished val_loss 0.2703447362780571, Time Elapse:  299.46469593199436 patient= 0\n",
      "Epoch 6 finished val_loss 0.25721187561750414, Time Elapse:  298.92640976101393 patient= 0\n",
      "Epoch 7 finished val_loss 0.24670373722910882, Time Elapse:  299.6531152910029 patient= 0\n",
      "Epoch 8 finished val_loss 0.24230074837803842, Time Elapse:  299.26525859499816 patient= 0\n",
      "Epoch 9 finished val_loss 0.24479462236166, Time Elapse:  299.2090156299819 patient= 1\n",
      "Epoch 10 finished val_loss 0.23737391844391822, Time Elapse:  299.3180188690021 patient= 0\n",
      "Epoch 11 finished val_loss 0.2292529022693634, Time Elapse:  298.7557704509818 patient= 0\n",
      "Epoch 12 finished val_loss 0.22762108072638512, Time Elapse:  300.4498411800014 patient= 0\n",
      "Epoch 13 finished val_loss 0.22675811842083932, Time Elapse:  298.5936430709844 patient= 0\n",
      "Epoch 14 finished val_loss 0.22333926126360892, Time Elapse:  300.33252103000996 patient= 0\n",
      "Epoch 15 finished val_loss 0.22952840179204942, Time Elapse:  297.5340191409923 patient= 1\n",
      "Epoch 16 finished val_loss 0.22030769154429436, Time Elapse:  299.6196360439935 patient= 0\n",
      "Epoch 17 finished val_loss 0.21794142842292785, Time Elapse:  299.00229536599363 patient= 0\n",
      "Epoch 18 finished val_loss 0.21578077122569084, Time Elapse:  299.2171040090034 patient= 0\n",
      "Epoch 19 finished val_loss 0.21600592404603958, Time Elapse:  298.1364460399782 patient= 1\n",
      "Epoch 20 finished val_loss 0.22154225066304206, Time Elapse:  299.04667373001575 patient= 2\n",
      "Epoch 21 finished val_loss 0.22164786010980606, Time Elapse:  298.13472820000607 patient= 3\n",
      "Epoch 22 finished val_loss 0.21919826090335845, Time Elapse:  298.7058136370033 patient= 4\n",
      "Epoch 23 finished val_loss 0.21287942588329314, Time Elapse:  298.72288483401644 patient= 0\n",
      "Epoch 24 finished val_loss 0.2152099519968033, Time Elapse:  299.49704159999965 patient= 1\n",
      "Epoch 25 finished val_loss 0.21725227072834968, Time Elapse:  299.19310440897243 patient= 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 finished val_loss 0.2139306089282036, Time Elapse:  300.275279649999 patient= 3\n",
      "Epoch 27 finished val_loss 0.2143271568417549, Time Elapse:  298.2768166780006 patient= 4\n",
      "Epoch 28 finished val_loss 0.21234393671154975, Time Elapse:  301.38425462701707 patient= 0\n",
      "Epoch 29 finished val_loss 0.2131918476521969, Time Elapse:  300.769796787994 patient= 1\n",
      "Epoch 30 finished val_loss 0.2144676199555397, Time Elapse:  299.14139427701593 patient= 2\n",
      "Epoch 31 finished val_loss 0.21339085429906846, Time Elapse:  299.54804418899585 patient= 3\n",
      "Epoch 32 finished val_loss 0.21703603163361548, Time Elapse:  299.2857346330129 patient= 4\n",
      "Epoch 33 finished val_loss 0.20990126475691795, Time Elapse:  298.97640593600227 patient= 0\n",
      "Epoch 34 finished val_loss 0.21767018377780914, Time Elapse:  298.7326886130031 patient= 1\n",
      "Epoch 35 finished val_loss 0.21504899755120277, Time Elapse:  299.865395522007 patient= 2\n",
      "Epoch 36 finished val_loss 0.21057791501283646, Time Elapse:  298.79788302700035 patient= 3\n",
      "Epoch 37 finished val_loss 0.21708621621131896, Time Elapse:  299.34019488398917 patient= 4\n",
      "Epoch 38 finished val_loss 0.21021091803908348, Time Elapse:  307.6805422909965 patient= 5\n",
      "Epoch 39 finished val_loss 0.2096383488178253, Time Elapse:  310.15939953699126 patient= 0\n",
      "Epoch 40 finished val_loss 0.21120343923568727, Time Elapse:  309.211258077994 patient= 1\n",
      "Epoch 41 finished val_loss 0.21027322009205818, Time Elapse:  310.08545232200413 patient= 2\n",
      "Epoch 42 finished val_loss 0.21267059192061424, Time Elapse:  308.6100968370156 patient= 3\n",
      "Epoch 43 finished val_loss 0.21045620769262313, Time Elapse:  309.7524930249783 patient= 4\n",
      "Epoch 44 finished val_loss 0.21216325491666793, Time Elapse:  307.59140603998094 patient= 5\n",
      "Epoch 45 finished val_loss 0.20911084577441216, Time Elapse:  308.43715257997974 patient= 0\n",
      "Epoch 46 finished val_loss 0.21126227840781212, Time Elapse:  307.14883820101386 patient= 1\n",
      "Epoch 47 finished val_loss 0.20924673914909364, Time Elapse:  308.42327962399577 patient= 2\n",
      "Epoch 48 finished val_loss 0.2117561364173889, Time Elapse:  310.23433942801785 patient= 3\n",
      "Epoch 49 finished val_loss 0.20920645147562028, Time Elapse:  301.8924136540154 patient= 4\n",
      "Epoch 50 finished val_loss 0.21073923408985137, Time Elapse:  299.7746339000005 patient= 5\n",
      "Epoch 51 finished val_loss 0.2105713403224945, Time Elapse:  299.5301940399804 patient= 6\n",
      "      current mse: 1.1350490255211811\n",
      "Total Epoch 150\n",
      " min_val_id 45, min_val_loss 0.20911084577441216, test_mse 1.1350490255211811\n",
      "nbjbuszzgc\n",
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (150, 1500, 0.01, 140, 2, True)\n",
      "EPOCH150BATCH_SIZE1500LR0.01N_HIDDEN_UNITS140NUM_LAYERS2BI_DIRTrue.pt\n",
      "Epoch 0 finished val_loss 0.40891049027442933, Time Elapse:  347.15492506101145 patient= 0\n",
      "Epoch 1 finished val_loss 0.366103792488575, Time Elapse:  339.5500644840067 patient= 0\n",
      "Epoch 2 finished val_loss 0.32978658616542816, Time Elapse:  339.02503341500415 patient= 0\n",
      "Epoch 3 finished val_loss 0.3076309138536453, Time Elapse:  338.98215594899375 patient= 0\n",
      "Epoch 4 finished val_loss 0.27990722030401227, Time Elapse:  336.5704036550014 patient= 0\n",
      "Epoch 5 finished val_loss 0.2632692897319794, Time Elapse:  335.64864942998975 patient= 0\n",
      "Epoch 6 finished val_loss 0.25284472838044164, Time Elapse:  336.25120254198555 patient= 0\n",
      "Epoch 7 finished val_loss 0.25211594298481943, Time Elapse:  335.6676485679927 patient= 0\n",
      "Epoch 8 finished val_loss 0.23692635655403138, Time Elapse:  335.4103188639856 patient= 0\n",
      "Epoch 9 finished val_loss 0.23564278319478035, Time Elapse:  335.0273894370184 patient= 0\n",
      "Epoch 10 finished val_loss 0.23114178732037544, Time Elapse:  337.0820807380078 patient= 0\n",
      "Epoch 11 finished val_loss 0.22749604761600495, Time Elapse:  341.2749053629814 patient= 0\n",
      "Epoch 12 finished val_loss 0.22241377472877502, Time Elapse:  342.10244364102255 patient= 0\n",
      "Epoch 13 finished val_loss 0.22075994580984115, Time Elapse:  345.11514518698095 patient= 0\n",
      "Epoch 14 finished val_loss 0.22136838585138321, Time Elapse:  344.20367332099704 patient= 1\n",
      "Epoch 15 finished val_loss 0.22005751699209214, Time Elapse:  342.44907553101075 patient= 0\n",
      "Epoch 16 finished val_loss 0.21829754292964934, Time Elapse:  341.5663424109807 patient= 0\n",
      "Epoch 17 finished val_loss 0.21755778938531875, Time Elapse:  340.7053038759914 patient= 0\n",
      "Epoch 18 finished val_loss 0.2172694520652294, Time Elapse:  341.64810500701424 patient= 0\n"
     ]
    }
   ],
   "source": [
    "hyper_paras_list = [\n",
    "    [150, 1500, 0.01, 90, 2, True],\n",
    "    [150, 1500, 0.01, 120, 2, True],\n",
    "    [150, 1500, 0.01, 140, 2, True]\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\",\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "\n",
    "for paras in hyper_paras_list:\n",
    "    EPOCH,BATCH_SIZE,LR,N_HIDDEN_UNITS,NUM_LAYERS, BI_DIR = paras\n",
    "    \n",
    "    model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "    model = model.float()\n",
    "    model.to(device)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "    \n",
    "#     train_dataset, val_dataset, test_dataset = train_dataset.to(device), val_dataset.to(device), test_dataset.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    print(f\"EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = {EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, BI_DIR}\")\n",
    "    \n",
    "    min_val_loss = 1e8\n",
    "    lowest_epoch = 0\n",
    "    PATH = f\"EPOCH{EPOCH}BATCH_SIZE{BATCH_SIZE}LR{LR}N_HIDDEN_UNITS{N_HIDDEN_UNITS}NUM_LAYERS{NUM_LAYERS}BI_DIR{BI_DIR}.pt\"\n",
    "    print(PATH)\n",
    "    \n",
    "    patient = 0\n",
    "    \n",
    "    for ix_epoch in range(EPOCH):\n",
    "#         print(f\"Epoch {ix_epoch} \")\n",
    "        start = timeit.default_timer()\n",
    "        train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "        val_loss = eval_model(val_loader, model, loss_function)\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        if val_loss < min_val_loss:\n",
    "                if os.path.exists(PATH):\n",
    "                    os.remove(PATH)\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "                lowest_epoch = ix_epoch\n",
    "                patient = 0\n",
    "                \n",
    "        elif val_loss > min_val_loss * 2:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            patient += 1\n",
    "            \n",
    "        \n",
    "        stop = timeit.default_timer()\n",
    "        print(f'Epoch {ix_epoch} finished val_loss {val_loss}, Time Elapse: ', stop - start, \"patient=\", patient) \n",
    "        \n",
    "        if patient >= 6:\n",
    "            break\n",
    "        \n",
    "    min_val_loss_id = np.argmin(val_loss_list)\n",
    "    min_val_loss = val_loss_list[min_val_loss_id]\n",
    "    \n",
    "    model = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    energy_predict = predict(test_loader, model)\n",
    "    test_mse = get_avg_mse(test_df[\"energy\"].values,energy_predict)\n",
    "    \n",
    "    print(f\"Total Epoch {EPOCH}\\n min_val_id {min_val_loss_id}, min_val_loss {min_val_loss}, test_mse {test_mse}\")\n",
    "\n",
    "    model_result = [EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS,BI_DIR, DATA_USED_N,min_val_loss_id, min_val_loss, test_mse, train_loss_list, val_loss_list]\n",
    "\n",
    "    current_df = pd.DataFrame([model_result], columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"BI_DIR\",\"data_used_n\" ,\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "    result_df = pd.concat([result_df,current_df])\n",
    "    letters = string.ascii_lowercase\n",
    "    suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "    print(suffix)\n",
    "    result_df[\"note\"] = \"multi layers for output neural network with 2 grains feature engineering\"\n",
    "    result_df.to_csv(f\"2022-05-01_1_{suffix}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd95728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7445d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2be8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283fd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc4a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d3330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ddb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd525602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792dcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
