{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e3f351c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_652332/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f5709a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_seed = 42\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, save\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "833f2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_log = []\n",
    "\n",
    "DNA_ONE_HOT = {'A': [0, 0, 0, 1],\n",
    "               'C': [0, 0, 1, 0],\n",
    "               'G': [0, 1, 0, 0],\n",
    "               'T': [1, 0, 0, 0]\n",
    "               }\n",
    "\n",
    "\n",
    "\n",
    "PADDING = True \n",
    "    \n",
    "MAX_LEN = 60\n",
    "N_FEATURES = 12\n",
    "\n",
    "DATA_USED_N = 1000000 # max is 1000000\n",
    "# DATA_USED_N = 10 # max is 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8c7e8824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data_org)= 1000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strand</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA...</td>\n",
       "      <td>-5.329171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT</td>\n",
       "      <td>-1.687614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG...</td>\n",
       "      <td>-7.224524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG</td>\n",
       "      <td>-7.222121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTCGGGAAGCAAAACCCCTAG</td>\n",
       "      <td>-1.601845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              strand    energy\n",
       "0  GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA... -5.329171\n",
       "1               AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT -1.687614\n",
       "2  CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG... -7.224524\n",
       "3             CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG -7.222121\n",
       "4                              CTCGGGAAGCAAAACCCCTAG -1.601845"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_org = pd.read_csv(\"./src/variable_length_dataset.csv\", names=[\"strand\",\"energy\"],dtype={0: str, 1: float})\n",
    "# data_org = pd.read_csv(\"./variable_length_dataset_1M.csv\", names=[\"strand\",\"energy\"],dtype={0: str, 1: float})\n",
    "data_org = pd.read_csv(\"./data/1mill_dataset.csv\", names=[\"strand\",\"energy\"],dtype={0: str, 1: float})\n",
    "print(\"len(data_org)=\",len(data_org))\n",
    "data_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d6098d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1000000, train idx 0 - 699999, val idx700000 - 849999, test idx 850000 - 999999\n",
      "\n",
      "\n",
      "len(train_df) : 700000, \n",
      "train_df_all(with reverse) 1400000 \n",
      "len(val_df) : 150000,\n",
      "len(test_df) : 150000\n"
     ]
    }
   ],
   "source": [
    "shuffled_df = data_org.sample(frac = 1, random_state=pandas_seed)\n",
    "shuffled_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# # Clip data, too large can not train in local\n",
    "\n",
    "shuffled_df = shuffled_df.loc[:DATA_USED_N-1]\n",
    "\n",
    "\n",
    "train_pct = 0.7\n",
    "val_pct = 0.15\n",
    "test_pct = 1 - train_pct - val_pct\n",
    "\n",
    "# # shuffled_df.head()\n",
    "all_n = len(shuffled_df)\n",
    "split1 = int(train_pct * all_n) \n",
    "split2 = int((train_pct + val_pct) * all_n) \n",
    "\n",
    "train_df_org = shuffled_df.loc[0:split1-1].copy()\n",
    "val_df = shuffled_df.loc[split1:split2-1].copy()\n",
    "test_df = shuffled_df.loc[split2:,:].copy()\n",
    "\n",
    "train_df_reverse = train_df_org.copy()\n",
    "train_df_reverse[\"strand\"] = train_df_reverse.loc[:,'strand'].apply(lambda x: x[::-1])\n",
    "\n",
    "train_df_all = pd.concat([train_df_org, train_df_reverse])\n",
    "\n",
    "print(f\"total {all_n}, train idx 0 - {split1-1}, val idx{split1} - {split2-1}, test idx {split2} - {all_n-1}\")\n",
    "print(f\"\\n\\nlen(train_df) : {len(train_df_org) }, \\ntrain_df_all(with reverse) {len(train_df_all)} \\nlen(val_df) : {len(val_df)},\\nlen(test_df) : {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df = train_df_all\n",
    "\n",
    "train_val_df = shuffled_df.loc[:split2-1]\n",
    "train_val_mean = train_val_df[\"energy\"].mean()\n",
    "train_val_stdev = train_val_df[\"energy\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "329ad780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def x_transform(dna_str, pad=PADDING):\n",
    "#     n_pad = MAX_LEN - len(dna_str) \n",
    "#     target = np.array(list(dna_str))\n",
    "#     onehot = np.array([DNA_ONE_HOT[letter] for letter in dna_str])\n",
    "# #     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     if pad == True:\n",
    "#         return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     else:\n",
    "#         return onehot          \n",
    "dict_LL_val = {\n",
    "    \"AA\":[1,1,1,1],\n",
    "    \"AG\":[1,1,1,0],\n",
    "    \"GA\":[1,1,1,0],\n",
    "    \"AC\":[1,1,0,1],\n",
    "    \"CA\":[1,1,0,1],\n",
    "    \"AT\":[1,1,0,0],\n",
    "    \"TA\":[1,1,0,0],\n",
    "    \"GG\":[1,0,1,1],\n",
    "    \"GC\":[1,0,1,0],\n",
    "    \"CG\":[1,0,1,0],\n",
    "    \"GT\":[1,0,0,1],\n",
    "    \"TG\":[1,0,0,1],\n",
    "    \"CC\":[1,0,0,0],\n",
    "    \"CT\":[0,1,1,1],\n",
    "    \"TC\":[0,1,1,1],\n",
    "    \"TT\":[0,1,1,0]\n",
    "}\n",
    "\n",
    "def count_A(str1):\n",
    "    return str1.count(\"A\")\n",
    "\n",
    "def count_G(str1):\n",
    "    return str1.count(\"G\")\n",
    "\n",
    "def count_C(str1):\n",
    "    return str1.count(\"C\")\n",
    "\n",
    "def count_T(str1):\n",
    "    return str1.count(\"T\")    \n",
    "\n",
    "def get_feature(str1):\n",
    "    alln = len(str1)\n",
    "    a,b,c,d = dict_LL_val[str1[0]+str1[-1]]\n",
    "    an = count_A(str1)\n",
    "    tn = count_T(str1)\n",
    "    cn = count_C(str1)\n",
    "    gn = count_G(str1)\n",
    "    rs = np.array([alln/MAX_LEN, an/alln, tn/alln, cn/alln, a,b,c,d]).reshape((1,8))\n",
    "    return np.tile(rs, (alln,1))\n",
    "\n",
    "def x_transform(dna_str, pad=PADDING):\n",
    "    \n",
    "    fea = get_feature(dna_str)\n",
    "    \n",
    "    n_pad = MAX_LEN - len(dna_str) \n",
    "    target = np.array(list(dna_str))\n",
    "    onehot = np.array([DNA_ONE_HOT[letter] for letter in dna_str])\n",
    "#     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     print(onehot.shape, fea.shape)\n",
    "    if pad == True:\n",
    "        return np.pad(np.concatenate((onehot,fea), axis=1),((0,n_pad),(0,0)), mode='constant')\n",
    "    else:\n",
    "        return np.concatenate((onehot,fea), axis=1) \n",
    "    \n",
    "    \n",
    "def y_transform(energy_float):\n",
    "    y_trans = (energy_float - train_val_mean) / train_val_stdev\n",
    "    return y_trans   \n",
    "\n",
    "def y_transform_reverce(y_array):\n",
    "    y_arr = y_array.cpu().numpy()\n",
    "    y_pred = y_arr * train_val_stdev + train_val_mean\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "716d1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_transform(\"ACCTT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01e919eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, df, x_transform, y_transform):\n",
    "        self.x_transform = x_transform\n",
    "        self.y_transform = y_transform\n",
    "        self.X = df[\"strand\"].values\n",
    "        self.y = df[\"energy\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        x = self.x_transform(self.X[idx])\n",
    "        y = self.y_transform(self.y[idx])\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# train_dataset = GetData(train_df, x_transform, y_transform)\n",
    "# X,y = train_dataset[1]\n",
    "# print(X.shape)\n",
    "# print(y)\n",
    "\n",
    "train_dataset = TransformDataset(train_df, x_transform, y_transform)\n",
    "val_dataset = TransformDataset(val_df, x_transform, y_transform)\n",
    "test_dataset = TransformDataset(test_df, x_transform, y_transform)\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# X,y = next(iter(train_loader))\n",
    "# print(\"Features shape:\", X.shape)\n",
    "# print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1d285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d342edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_DNA(nn.Module):\n",
    "    def __init__(self, input_dim_n, hidden_units, num_layers,is_bi_dir):\n",
    "        super(LSTM_DNA, self).__init__()\n",
    "        self.input_dim = input_dim_n\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.direction_n = 1 + int(is_bi_dir)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_dim_n,\n",
    "            hidden_size = hidden_units,\n",
    "            batch_first = True,\n",
    "            num_layers = self.num_layers,\n",
    "            bidirectional = is_bi_dir\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(in_features = self.hidden_units, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten()\n",
    "        \n",
    "        return out\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         loss = ...\n",
    "#         self.log(\"val_loss\", loss)\n",
    "    \n",
    "# LR = 5e-5\n",
    "# N_HIDDEN_UNITS = 16\n",
    "# model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS)\n",
    "# model = model.float()\n",
    "# loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72647d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "00466069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    batch_i = 1\n",
    "    model = model.float()\n",
    "    for X, y in data_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X.float())\n",
    "        loss = loss_function(output, y.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_i += 1\n",
    "#         if batch_i % 200 == 0:\n",
    "#             print(f\"batch {batch_i}, loss = {loss.item()}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches \n",
    "#     print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "def eval_model(data_loader, model, loss_function):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X.float())\n",
    "            total_loss += loss_function(output, y.float()).item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "#     print(f\"   Evalulation loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "\n",
    "def predict(data_loader, model):\n",
    "\n",
    "    output = torch.tensor([])\n",
    "    output = output.to(device)\n",
    "    model.eval()\n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            X = X.to(device)\n",
    "            y_out = model(X.float())\n",
    "            output = torch.cat((output, y_out.float()), 0)\n",
    "    \n",
    "    energy = y_transform_reverce(output)\n",
    "    \n",
    "    return energy\n",
    "\n",
    "def get_avg_mse(y, y_pred):\n",
    "    mse = ((y - y_pred)**2).mean()\n",
    "    print(f\"      current mse: {mse}\")\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30a49b75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (150, 2500, 0.01, 90, 1, True)\n",
      "EPOCH150BATCH_SIZE2500LR0.01N_HIDDEN_UNITS90NUM_LAYERS90BI_DIRTrue.pt\n",
      "Epoch 0 finished val_loss0.44219367106755575, Time Elapse:  209.50759430299513\n",
      "Epoch 1 finished val_loss0.41864496817191443, Time Elapse:  210.45606882002903\n",
      "Epoch 2 finished val_loss0.36869360903898873, Time Elapse:  209.99324155796785\n",
      "Epoch 3 finished val_loss0.33857081482807794, Time Elapse:  208.82277110399446\n",
      "Epoch 4 finished val_loss0.318447944521904, Time Elapse:  209.32650022098096\n",
      "Epoch 5 finished val_loss0.3070462812980016, Time Elapse:  208.06348023598548\n",
      "Epoch 6 finished val_loss0.2967433621486028, Time Elapse:  207.0351848829887\n",
      "Epoch 7 finished val_loss0.28532771418492, Time Elapse:  207.17792941094376\n",
      "Epoch 8 finished val_loss0.27970562428236007, Time Elapse:  208.1045995859895\n",
      "Epoch 9 finished val_loss0.27352520227432253, Time Elapse:  206.15505207394017\n",
      "Epoch 10 finished val_loss0.26796905547380445, Time Elapse:  206.3739239219576\n",
      "Epoch 11 finished val_loss0.26463982264200847, Time Elapse:  207.18769961595535\n",
      "Epoch 12 finished val_loss0.26060959448417026, Time Elapse:  207.64551324909553\n",
      "Epoch 13 finished val_loss0.2537355855107307, Time Elapse:  207.2054562260164\n",
      "Epoch 14 finished val_loss0.24965989241997402, Time Elapse:  207.69971132196952\n",
      "Epoch 15 finished val_loss0.25453292081753415, Time Elapse:  207.66788800398353\n",
      "Epoch 16 finished val_loss0.24913368945320447, Time Elapse:  208.526083452045\n",
      "Epoch 17 finished val_loss0.24509791483481724, Time Elapse:  206.54611886898056\n",
      "Epoch 18 finished val_loss0.24484599952896435, Time Elapse:  207.0995924359886\n",
      "Epoch 19 finished val_loss0.24152174467841783, Time Elapse:  207.50273551605642\n",
      "Epoch 20 finished val_loss0.24980774174133938, Time Elapse:  207.1501095109852\n",
      "Epoch 21 finished val_loss0.23952704618374507, Time Elapse:  206.6162709039636\n",
      "Epoch 22 finished val_loss0.24160676474372547, Time Elapse:  206.53481803496834\n",
      "Epoch 23 finished val_loss0.237096622834603, Time Elapse:  206.5961936690146\n",
      "Epoch 24 finished val_loss0.23763873030742008, Time Elapse:  206.52886423002928\n",
      "Epoch 25 finished val_loss0.23644908542434376, Time Elapse:  206.92816085892264\n",
      "Epoch 26 finished val_loss0.23916835909088452, Time Elapse:  208.17523970105685\n",
      "Epoch 27 finished val_loss0.23561963041623432, Time Elapse:  206.26389329496305\n",
      "Epoch 28 finished val_loss0.23938772057493526, Time Elapse:  207.35048581601586\n",
      "Epoch 29 finished val_loss0.23719082449873288, Time Elapse:  205.1420012731105\n",
      "Epoch 30 finished val_loss0.23302451968193055, Time Elapse:  206.76487103197724\n",
      "Epoch 31 finished val_loss0.23290138964851698, Time Elapse:  207.29223030305002\n",
      "Epoch 32 finished val_loss0.2370817338426908, Time Elapse:  206.8097244759556\n",
      "Epoch 33 finished val_loss0.23408589884638786, Time Elapse:  206.49280688597355\n",
      "Epoch 34 finished val_loss0.23519134223461152, Time Elapse:  206.88491232297383\n",
      "Epoch 35 finished val_loss0.23400862415631613, Time Elapse:  206.3720805319026\n",
      "Epoch 36 finished val_loss0.2334907755255699, Time Elapse:  205.62784443900455\n",
      "Epoch 37 finished val_loss0.2319471299648285, Time Elapse:  206.78494373802096\n",
      "Epoch 38 finished val_loss0.23287542512019474, Time Elapse:  207.2034466500627\n",
      "Epoch 39 finished val_loss0.23223492627342543, Time Elapse:  207.8934114529984\n",
      "Epoch 40 finished val_loss0.22973010738690694, Time Elapse:  207.1918954050634\n",
      "Epoch 41 finished val_loss0.22844417293866476, Time Elapse:  207.06782094202936\n",
      "Epoch 42 finished val_loss0.23011725569764774, Time Elapse:  208.6517761159921\n",
      "Epoch 43 finished val_loss0.23387313882509866, Time Elapse:  207.47189927706495\n",
      "Epoch 44 finished val_loss0.2315692866841952, Time Elapse:  205.84146169107407\n",
      "Epoch 45 finished val_loss0.23032074520985285, Time Elapse:  206.83863923500758\n",
      "Epoch 46 finished val_loss0.2278497596581777, Time Elapse:  207.11748637002893\n",
      "Epoch 47 finished val_loss0.22896117145816486, Time Elapse:  205.80943009909242\n",
      "Epoch 48 finished val_loss0.22660415743788084, Time Elapse:  205.62433457095176\n",
      "Epoch 49 finished val_loss0.22776026849945386, Time Elapse:  208.76533397298772\n",
      "Epoch 50 finished val_loss0.22802225053310393, Time Elapse:  208.5876327699516\n",
      "Epoch 51 finished val_loss0.227249859025081, Time Elapse:  207.4028583059553\n",
      "Epoch 52 finished val_loss0.22975456217924753, Time Elapse:  207.4995660269633\n",
      "Epoch 53 finished val_loss0.2272540179391702, Time Elapse:  208.37800917692948\n",
      "Epoch 54 finished val_loss0.22763124778866767, Time Elapse:  207.98913475801237\n",
      "Epoch 55 finished val_loss0.2269201417764028, Time Elapse:  206.29957799392287\n",
      "Epoch 56 finished val_loss0.22860388457775116, Time Elapse:  205.4754355000332\n",
      "Epoch 57 finished val_loss0.2290159558256467, Time Elapse:  207.59672835399397\n",
      "Epoch 58 finished val_loss0.22541189243396123, Time Elapse:  206.96146484604105\n",
      "Epoch 59 finished val_loss0.22861403400699298, Time Elapse:  207.83640004403424\n",
      "Epoch 60 finished val_loss0.22849336018164954, Time Elapse:  208.59477006504312\n",
      "Epoch 61 finished val_loss0.2321600464483102, Time Elapse:  208.531998968916\n",
      "Epoch 62 finished val_loss0.22633116568128267, Time Elapse:  206.92686192609835\n",
      "Epoch 63 finished val_loss0.22545288850863773, Time Elapse:  206.32453794695903\n",
      "Epoch 64 finished val_loss0.2251653236647447, Time Elapse:  204.80024294008035\n",
      "Epoch 65 finished val_loss0.22589435453216236, Time Elapse:  205.01598389307037\n",
      "Epoch 66 finished val_loss0.22707427988449733, Time Elapse:  205.78229693497997\n",
      "Epoch 67 finished val_loss0.22560501073797543, Time Elapse:  205.93855206598528\n",
      "Epoch 68 finished val_loss0.22557549277941386, Time Elapse:  206.95814624696504\n",
      "Epoch 69 finished val_loss0.2241783748070399, Time Elapse:  206.34840731497388\n",
      "Epoch 70 finished val_loss0.22661770284175872, Time Elapse:  207.25316410406958\n",
      "Epoch 71 finished val_loss0.22535054807861646, Time Elapse:  205.8331865099026\n",
      "Epoch 72 finished val_loss0.23461709221204122, Time Elapse:  206.46529664797708\n",
      "Epoch 73 finished val_loss0.2257104476292928, Time Elapse:  207.77938159706537\n",
      "Epoch 74 finished val_loss0.2266120101014773, Time Elapse:  207.29143529501744\n",
      "Epoch 75 finished val_loss0.22503733014067015, Time Elapse:  207.65758700598963\n",
      "Epoch 76 finished val_loss0.2268451377749443, Time Elapse:  206.48106315196492\n",
      "Epoch 77 finished val_loss0.2253525753815969, Time Elapse:  206.60753072600346\n",
      "Epoch 78 finished val_loss0.22556744515895844, Time Elapse:  207.70672607806046\n",
      "Epoch 79 finished val_loss0.2261045346657435, Time Elapse:  206.52982531196903\n",
      "      current mse: 1.2201605708323806\n",
      "Total Epoch 150\n",
      " min_val_id 69, min_val_loss 0.2241783748070399, test_mse 1.2201605708323806\n",
      "awdljttzhu\n"
     ]
    }
   ],
   "source": [
    "# for BATCH_SIZE in [1500]:\n",
    "#     for LR in [1e-2]:\n",
    "#         for N_HIDDEN_UNITS in [12]:\n",
    "#             for NUM_LAYERS in [1]:   150\t2500\t0.01\t60\t1\t1000000\t\n",
    "# hyper_paras_list = [\n",
    "#     [150, 2500, 0.01, 90, 1, True]\n",
    "# ]\n",
    "\n",
    "hyper_paras_list = [\n",
    "    [150, 2500, 0.01, 90, 1, True]\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\",\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "\n",
    "for paras in hyper_paras_list:\n",
    "    EPOCH,BATCH_SIZE,LR,N_HIDDEN_UNITS,NUM_LAYERS, BI_DIR = paras\n",
    "    \n",
    "    model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "    model = model.float()\n",
    "    model.to(device)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "    \n",
    "#     train_dataset, val_dataset, test_dataset = train_dataset.to(device), val_dataset.to(device), test_dataset.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    print(f\"EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = {EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, BI_DIR}\")\n",
    "    \n",
    "    min_val_loss = 1e8\n",
    "    lowest_epoch = 0\n",
    "    PATH = f\"EPOCH{EPOCH}BATCH_SIZE{BATCH_SIZE}LR{LR}N_HIDDEN_UNITS{N_HIDDEN_UNITS}NUM_LAYERS{N_HIDDEN_UNITS}BI_DIR{BI_DIR}.pt\"\n",
    "    print(PATH)\n",
    "    \n",
    "    patient = 0\n",
    "    \n",
    "    for ix_epoch in range(EPOCH):\n",
    "#         print(f\"Epoch {ix_epoch} \")\n",
    "        start = timeit.default_timer()\n",
    "        train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "        val_loss = eval_model(val_loader, model, loss_function)\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        if val_loss < min_val_loss:\n",
    "                if os.path.exists(PATH):\n",
    "                    os.remove(PATH)\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "                lowest_epoch = ix_epoch\n",
    "                patient = 0\n",
    "        \n",
    "        else:\n",
    "            patient += 1\n",
    "        \n",
    "        stop = timeit.default_timer()\n",
    "        print(f'Epoch {ix_epoch} finished val_loss{val_loss}, Time Elapse: ', stop - start) \n",
    "        \n",
    "        if patient >= 10:\n",
    "            break\n",
    "        \n",
    "    min_val_loss_id = np.argmin(val_loss_list)\n",
    "    min_val_loss = val_loss_list[min_val_loss_id]\n",
    "    \n",
    "    energy_predict = predict(test_loader, model)\n",
    "    test_mse = get_avg_mse(test_df[\"energy\"].values,energy_predict)\n",
    "    \n",
    "    print(f\"Total Epoch {EPOCH}\\n min_val_id {min_val_loss_id}, min_val_loss {min_val_loss}, test_mse {test_mse}\")\n",
    "\n",
    "    model_result = [EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS,BI_DIR, DATA_USED_N,min_val_loss_id, min_val_loss, test_mse, train_loss_list, val_loss_list]\n",
    "\n",
    "    current_df = pd.DataFrame([model_result], columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"BI_DIR\",\"data_used_n\" ,\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "    result_df = pd.concat([result_df,current_df])\n",
    "    letters = string.ascii_lowercase\n",
    "    suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "    print(suffix)\n",
    "    result_df[\"note\"] = \"alln/MAX_LEN, an/alln, tn/alln, cn/alln, a,b,c,d] \"\n",
    "    result_df.to_csv(f\"2022-04-20_1_{suffix}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d8f7445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for BATCH_SIZE in [1500]:\n",
    "# #     for LR in [1e-2]:\n",
    "# #         for N_HIDDEN_UNITS in [12]:\n",
    "# #             for NUM_LAYERS in [1]:\n",
    "# hyper_paras_list = [\n",
    "#     [80, 2500, 0.01, 20, 1]\n",
    "# ]\n",
    "\n",
    "# result_df = pd.DataFrame(columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\",\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "\n",
    "# for paras in hyper_paras_list:\n",
    "#     EPOCH,BATCH_SIZE,LR,N_HIDDEN_UNITS,NUM_LAYERS = paras\n",
    "    \n",
    "#     model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS)\n",
    "#     model = model.float()\n",
    "#     model.to(device)\n",
    "#     loss_function = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "    \n",
    "# #     train_dataset, val_dataset, test_dataset = train_dataset.to(device), val_dataset.to(device), test_dataset.to(device)\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     model_list = []\n",
    "#     train_loss_list = []\n",
    "#     val_loss_list = []\n",
    "\n",
    "#     print(f\"EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS = {EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS}\")\n",
    "    \n",
    "#     min_val_loss = 1e8\n",
    "#     lowest_epoch = 0\n",
    "#     PATH = f\"EPOCH{EPOCH}BATCH_SIZE{BATCH_SIZE}LR{LR}N_HIDDEN_UNITS{N_HIDDEN_UNITS}NUM_LAYERS{N_HIDDEN_UNITS}.pt\"\n",
    "#     print(PATH)\n",
    "    \n",
    "    \n",
    "#     for ix_epoch in range(EPOCH):\n",
    "#         print(f\"Epoch {ix_epoch} \")\n",
    "#         start = timeit.default_timer()\n",
    "#         train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "#         val_loss = eval_model(val_loader, model, loss_function)\n",
    "        \n",
    "#         train_loss_list.append(train_loss)\n",
    "#         val_loss_list.append(val_loss)\n",
    "        \n",
    "#         if val_loss < min_val_loss:\n",
    "#                 if os.path.exists(PATH):\n",
    "#                     os.remove(PATH)\n",
    "#                 torch.save(model.state_dict(), PATH)\n",
    "#                 min_val_loss = val_loss\n",
    "#                 lowest_epoch = ix_epoch\n",
    "        \n",
    "#         stop = timeit.default_timer()\n",
    "#         print(f'Epoch finished val_loss{val_loss}, Time Elapse: ', stop - start) \n",
    "    \n",
    "#     min_val_loss_id = np.argmin(val_loss_list)\n",
    "#     min_val_loss = val_loss_list[min_val_loss_id]\n",
    "    \n",
    "#     energy_predict = predict(test_loader, model)\n",
    "#     test_mse = get_avg_mse(test_df[\"energy\"].values,energy_predict)\n",
    "    \n",
    "#     print(f\"Total Epoch {EPOCH}\\n min_val_id {min_val_loss_id}, min_val_loss {min_val_loss}, test_mse {test_mse}\")\n",
    "\n",
    "#     model_result = [EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, DATA_USED_N,min_val_loss_id, min_val_loss, test_mse, train_loss_list, val_loss_list]\n",
    "\n",
    "#     current_df = pd.DataFrame([model_result], columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\" ,\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "#     result_df = pd.concat([result_df,current_df])\n",
    "#     result_df.to_csv(\"2022-04-15_n.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c2be8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# printing lowercase\n",
    "# letters = string.ascii_lowercase\n",
    "# suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "# suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a8247c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def x_transform(dna_str, pad=True):\n",
    "#     n_pad = MAX_LEN - len(dna_str) \n",
    "#     target = np.array(list(dna_str))\n",
    "#     onehot = np.array([DNA_ONE_HOT[letter] for letter in dna_str])\n",
    "# #     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "    \n",
    "#     if pad == True:\n",
    "#         return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     else:\n",
    "#         return onehot\n",
    "\n",
    "# str1 = \"CGATT\"\n",
    "# x_transform(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4283fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_A(str1):\n",
    "#     return str1.count(\"A\")\n",
    "\n",
    "# def count_G(str1):\n",
    "#     return str1.count(\"G\")\n",
    "\n",
    "# def count_C(str1):\n",
    "#     return str1.count(\"C\")\n",
    "\n",
    "# def count_T(str1):\n",
    "#     return str1.count(\"T\")    \n",
    "\n",
    "# def get_feature(str1):\n",
    "#     alln = len(str1)\n",
    "#     an = count_A(str1)\n",
    "#     tn = count_T(str1)\n",
    "#     cn = count_C(str1)\n",
    "#     gn = count_G(str1)\n",
    "#     rs = np.array([alln, an, tn, cn, gn]).reshape((1,5))\n",
    "#     return np.tile(rs, (alln,1))\n",
    "\n",
    "# def x_transform(dna_str, pad=PADDING):\n",
    "    \n",
    "#     fea = get_feature(dna_str)\n",
    "    \n",
    "#     n_pad = MAX_LEN - len(dna_str) \n",
    "#     target = np.array(list(dna_str))\n",
    "#     onehot = np.array([DNA_ONE_HOT[letter] for letter in dna_str])\n",
    "# #     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     print(onehot.shape, fea.shape)\n",
    "#     if pad == True:\n",
    "#         return np.pad(np.concatenate((onehot,fea), axis=1),((0,n_pad),(0,0)), mode='constant'), fea\n",
    "#     else:\n",
    "#         return np.concatenate((onehot,fea), axis=1) \n",
    "\n",
    "# # (np.array([DNA_ONE_HOT[letter]) for letter in dna_str]), get_feature(dna_str).reshape((1, 5))), axis=1)\n",
    "# #     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "# #     if pad == True:\n",
    "# #         return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "# #     else:\n",
    "# #         return onehot \n",
    "\n",
    "\n",
    "# n1 = get_feature(\"ATCG\")\n",
    "# n1.shape\n",
    "# x_transform(\"ATCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c240fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([1,2,3]).reshape((-1,3))\n",
    "# print(x.shape)\n",
    "# np.tile(x, (4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "676946f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_feature(\"CGATTTYA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0ac296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmark = pd.DataFrame([\"done\"],columns=[\"is_done\"])\n",
    "dfmark.to_csv(\"is_done.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc4a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d3330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ddb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd525602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792dcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
