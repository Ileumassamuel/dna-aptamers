{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e3f351c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_576890/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f5709a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_seed = 42\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, save\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "833f2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_log = []\n",
    "\n",
    "DNA_ONE_HOT = {'A': [0, 0, 0, 1],\n",
    "               'C': [0, 0, 1, 0],\n",
    "               'G': [0, 1, 0, 0],\n",
    "               'T': [1, 0, 0, 0]\n",
    "               }\n",
    "\n",
    "\n",
    "\n",
    "PADDING = True \n",
    "    \n",
    "MAX_LEN = 60\n",
    "N_FEATURES = 4\n",
    "N_percentage = 0.25 # 1 - 1M\n",
    "\n",
    "# DATA_USED_N = 1000000 # max is 1000000\n",
    "DATA_USED_N = 60 # max is 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8c7e8824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data_org)= 1000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strand</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA...</td>\n",
       "      <td>-5.329171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT</td>\n",
       "      <td>-1.687614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG...</td>\n",
       "      <td>-7.224524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG</td>\n",
       "      <td>-7.222121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTCGGGAAGCAAAACCCCTAG</td>\n",
       "      <td>-1.601845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              strand    energy\n",
       "0  GAATCTTCGCACTCTAGCTGACCGCCTTCAGTAGTACGAATCTGGA... -5.329171\n",
       "1               AAGAAGCGGGAAGCATATCTTTATTCAGTTCCTAAT -1.687614\n",
       "2  CCTAATCGGGAATTGTTCCTCTTCCATTTGTAATGGTTATAAGAGG... -7.224524\n",
       "3             CACCGGGGCATTTATCCCGGGCTCGAAGGAAGTCTTGG -7.222121\n",
       "4                              CTCGGGAAGCAAAACCCCTAG -1.601845"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_org = pd.read_csv(\"./src/variable_length_dataset.csv\", names=[\"strand\",\"energy\"],dtype={0: str, 1: float})\n",
    "# data_org = pd.read_csv(\"./variable_length_dataset_1M.csv\", names=[\"strand\",\"energy\"],dtype={0: str, 1: float})\n",
    "data_org = pd.read_csv(\"./data/1mill_dataset.csv\", names=[\"strand\",\"energy\"],dtype={0: str, 1: float})\n",
    "print(\"len(data_org)=\",len(data_org))\n",
    "data_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4d6098d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 60, train idx 0 - 41, val idx42 - 50, test idx 51 - 59\n",
      "\n",
      "\n",
      "len(train_df) : 42, \n",
      "train_df_all(with reverse) 84 \n",
      "len(val_df) : 9,\n",
      "len(test_df) : 9\n"
     ]
    }
   ],
   "source": [
    "shuffled_df = data_org.sample(frac = 1, random_state=pandas_seed)\n",
    "shuffled_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# # Clip data, too large can not train in local\n",
    "\n",
    "shuffled_df = shuffled_df.loc[:DATA_USED_N-1]\n",
    "\n",
    "\n",
    "train_pct = 0.7\n",
    "val_pct = 0.15\n",
    "test_pct = 1 - train_pct - val_pct\n",
    "\n",
    "# # shuffled_df.head()\n",
    "all_n = len(shuffled_df)\n",
    "split1 = int(train_pct * all_n) \n",
    "split2 = int((train_pct + val_pct) * all_n) \n",
    "\n",
    "train_df_org = shuffled_df.loc[0:split1-1].copy()\n",
    "val_df = shuffled_df.loc[split1:split2-1].copy()\n",
    "test_df = shuffled_df.loc[split2:,:].copy()\n",
    "\n",
    "train_df_reverse = train_df_org.copy()\n",
    "train_df_reverse[\"strand\"] = train_df_reverse.loc[:,'strand'].apply(lambda x: x[::-1])\n",
    "\n",
    "train_df_all = pd.concat([train_df_org, train_df_reverse])\n",
    "\n",
    "print(f\"total {all_n}, train idx 0 - {split1-1}, val idx{split1} - {split2-1}, test idx {split2} - {all_n-1}\")\n",
    "print(f\"\\n\\nlen(train_df) : {len(train_df_org) }, \\ntrain_df_all(with reverse) {len(train_df_all)} \\nlen(val_df) : {len(val_df)},\\nlen(test_df) : {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df = train_df_all\n",
    "\n",
    "train_val_df = shuffled_df.loc[:split2-1]\n",
    "train_val_mean = train_val_df[\"energy\"].mean()\n",
    "train_val_stdev = train_val_df[\"energy\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "329ad780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def x_transform(dna_str, pad=PADDING):\n",
    "#     n_pad = MAX_LEN - len(dna_str) \n",
    "#     target = np.array(list(dna_str))\n",
    "#     onehot = np.array([DNA_ONE_HOT[letter] for letter in dna_str])\n",
    "# #     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     if pad == True:\n",
    "#         return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     else:\n",
    "#         return onehot          \n",
    "\n",
    "\n",
    "# def count_A(str1):\n",
    "#     return str1.count(\"A\")\n",
    "\n",
    "# def count_G(str1):\n",
    "#     return str1.count(\"G\")\n",
    "\n",
    "# def count_C(str1):\n",
    "#     return str1.count(\"C\")\n",
    "\n",
    "# def count_T(str1):\n",
    "#     return str1.count(\"T\")    \n",
    "\n",
    "# def get_feature(str1):\n",
    "#     alln = len(str1)\n",
    "#     an = count_A(str1)\n",
    "#     tn = count_T(str1)\n",
    "#     cn = count_C(str1)\n",
    "#     gn = count_G(str1)\n",
    "#     rs = np.array([alln/MAX_LEN, an/alln, tn/alln, cn/alln, gn/alln]).reshape((1,5))\n",
    "#     return np.tile(rs, (alln,1))\n",
    "\n",
    "# def x_transform(dna_str, pad=PADDING):\n",
    "    \n",
    "#     fea = get_feature(dna_str)\n",
    "    \n",
    "#     n_pad = MAX_LEN - len(dna_str) \n",
    "#     target = np.array(list(dna_str))\n",
    "#     onehot = np.array([DNA_ONE_HOT[letter] for letter in dna_str])\n",
    "# #     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "# #     print(onehot.shape, fea.shape)\n",
    "#     if pad == True:\n",
    "#         return np.pad(np.concatenate((onehot,fea), axis=1),((0,n_pad),(0,0)), mode='constant')\n",
    "#     else:\n",
    "#         return np.concatenate((onehot,fea), axis=1) \n",
    "\n",
    "def x_transform(dna_str, pad=PADDING):\n",
    "    \n",
    "    \n",
    "    n_pad = MAX_LEN - len(dna_str) \n",
    "    target = np.array(list(dna_str))\n",
    "    onehot = np.array([DNA_ONE_HOT[letter] for letter in dna_str])\n",
    "#     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     print(onehot.shape, fea.shape)\n",
    "    if pad == True:\n",
    "        return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "    else:\n",
    "        return onehot\n",
    "    \n",
    "    \n",
    "def y_transform(energy_float):\n",
    "    y_trans = (energy_float - train_val_mean) / train_val_stdev\n",
    "    return y_trans   \n",
    "\n",
    "def y_transform_reverce(y_array):\n",
    "    y_arr = y_array.cpu().numpy()\n",
    "    y_pred = y_arr * train_val_stdev + train_val_mean\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "948b3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_transform(\"CGTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "01e919eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, df, x_transform, y_transform):\n",
    "        self.x_transform = x_transform\n",
    "        self.y_transform = y_transform\n",
    "        self.X = df[\"strand\"].values\n",
    "        self.y = df[\"energy\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        x = self.x_transform(self.X[idx])\n",
    "        y = self.y_transform(self.y[idx])\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# train_dataset = GetData(train_df, x_transform, y_transform)\n",
    "# X,y = train_dataset[1]\n",
    "# print(X.shape)\n",
    "# print(y)\n",
    "\n",
    "train_dataset = TransformDataset(train_df, x_transform, y_transform)\n",
    "val_dataset = TransformDataset(val_df, x_transform, y_transform)\n",
    "test_dataset = TransformDataset(test_df, x_transform, y_transform)\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# X,y = next(iter(train_loader))\n",
    "# print(\"Features shape:\", X.shape)\n",
    "# print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1d285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d342edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_DNA(nn.Module):\n",
    "    def __init__(self, input_dim_n, hidden_units, num_layers,is_bi_dir):\n",
    "        super(LSTM_DNA, self).__init__()\n",
    "        self.input_dim = input_dim_n\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.direction_n = (1 + int(is_bi_dir))\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_dim_n,\n",
    "            hidden_size = hidden_units,\n",
    "            batch_first = True,\n",
    "            num_layers = self.num_layers,\n",
    "            bidirectional = is_bi_dir\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features = self.hidden_units, out_features=45)\n",
    "        self.linear2 = nn.Linear(in_features = 45, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers * self.direction_n, batch_size, self.hidden_units).to(device).requires_grad_()\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "#         out1 = self.linear1(hn[0]).flatten()\n",
    "        out1 = self.linear1(hn[0])\n",
    "#         print(out1.shape)\n",
    "        out2 = self.linear2(out1).flatten()\n",
    "        \n",
    "        return out2\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         loss = ...\n",
    "#         self.log(\"val_loss\", loss)\n",
    "    \n",
    "# LR = 5e-5\n",
    "# N_HIDDEN_UNITS = 16\n",
    "# model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS)\n",
    "# model = model.float()\n",
    "# loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72647d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "00466069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    batch_i = 1\n",
    "    model = model.float()\n",
    "    for X, y in data_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X.float())\n",
    "        loss = loss_function(output, y.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_i += 1\n",
    "#         if batch_i % 200 == 0:\n",
    "#             print(f\"batch {batch_i}, loss = {loss.item()}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches \n",
    "#     print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "def eval_model(data_loader, model, loss_function):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X.float())\n",
    "            total_loss += loss_function(output, y.float()).item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "#     print(f\"   Evalulation loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "    \n",
    "\n",
    "def predict(data_loader, model):\n",
    "\n",
    "    output = torch.tensor([])\n",
    "    output = output.to(device)\n",
    "    model.eval()\n",
    "    model = model.float()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            X = X.to(device)\n",
    "            y_out = model(X.float())\n",
    "            output = torch.cat((output, y_out.float()), 0)\n",
    "    \n",
    "    energy = y_transform_reverce(output)\n",
    "    \n",
    "    return energy\n",
    "\n",
    "def get_avg_mse(y, y_pred):\n",
    "    mse = ((y - y_pred)**2).mean()\n",
    "    print(f\"      current mse: {mse}\")\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "30a49b75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = (3, 2, 0.01, 15, 1, True)\n",
      "EPOCH3BATCH_SIZE2LR0.01N_HIDDEN_UNITS15NUM_LAYERS15BI_DIRTrue.pt\n",
      "Epoch 0 finished val_loss3.2311180591583253, Time Elapse:  0.17067847098223865\n",
      "Epoch 1 finished val_loss3.1648032173514364, Time Elapse:  0.17120488500222564\n",
      "Epoch 2 finished val_loss3.058287987206131, Time Elapse:  0.16918102599447593\n",
      "      current mse: 1.9326486225921258\n",
      "Total Epoch 3\n",
      " min_val_id 2, min_val_loss 3.058287987206131, test_mse 1.9326486225921258\n",
      "hlstbjzhrw\n"
     ]
    }
   ],
   "source": [
    "# for BATCH_SIZE in [1500]:\n",
    "#     for LR in [1e-2]:\n",
    "#         for N_HIDDEN_UNITS in [12]:\n",
    "#             for NUM_LAYERS in [1]:   150\t2500\t0.01\t60\t1\t1000000\t\n",
    "# hyper_paras_list = [\n",
    "#     [150, 2500, 0.01, 90, 1, True]\n",
    "# ]\n",
    "\n",
    "hyper_paras_list = [\n",
    "    [3, 2, 0.01, 15, 1, True]\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\",\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "\n",
    "for paras in hyper_paras_list:\n",
    "    EPOCH,BATCH_SIZE,LR,N_HIDDEN_UNITS,NUM_LAYERS, BI_DIR = paras\n",
    "    \n",
    "    model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "    model = model.float()\n",
    "    model.to(device)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "    \n",
    "#     train_dataset, val_dataset, test_dataset = train_dataset.to(device), val_dataset.to(device), test_dataset.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    print(f\"EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, is_bi_dir = {EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, BI_DIR}\")\n",
    "    \n",
    "    min_val_loss = 1e8\n",
    "    lowest_epoch = 0\n",
    "    PATH = f\"EPOCH{EPOCH}BATCH_SIZE{BATCH_SIZE}LR{LR}N_HIDDEN_UNITS{N_HIDDEN_UNITS}NUM_LAYERS{N_HIDDEN_UNITS}BI_DIR{BI_DIR}.pt\"\n",
    "    print(PATH)\n",
    "    \n",
    "    patient = 0\n",
    "    \n",
    "    for ix_epoch in range(EPOCH):\n",
    "#         print(f\"Epoch {ix_epoch} \")\n",
    "        start = timeit.default_timer()\n",
    "        train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "        val_loss = eval_model(val_loader, model, loss_function)\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        if val_loss < min_val_loss:\n",
    "                if os.path.exists(PATH):\n",
    "                    os.remove(PATH)\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "                lowest_epoch = ix_epoch\n",
    "                patient = 0\n",
    "        \n",
    "        else:\n",
    "            patient += 1\n",
    "        \n",
    "        stop = timeit.default_timer()\n",
    "        print(f'Epoch {ix_epoch} finished val_loss{val_loss}, Time Elapse: ', stop - start) \n",
    "        \n",
    "        if patient >= 5:\n",
    "            break\n",
    "        \n",
    "    min_val_loss_id = np.argmin(val_loss_list)\n",
    "    min_val_loss = val_loss_list[min_val_loss_id]\n",
    "    \n",
    "    \n",
    "    model = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS, is_bi_dir=BI_DIR)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    energy_predict = predict(test_loader, model)\n",
    "    test_mse = get_avg_mse(test_df[\"energy\"].values,energy_predict)\n",
    "    \n",
    "    print(f\"Total Epoch {EPOCH}\\n min_val_id {min_val_loss_id}, min_val_loss {min_val_loss}, test_mse {test_mse}\")\n",
    "\n",
    "    model_result = [EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS,BI_DIR, DATA_USED_N,min_val_loss_id, min_val_loss, test_mse, train_loss_list, val_loss_list]\n",
    "\n",
    "    current_df = pd.DataFrame([model_result], columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"BI_DIR\",\"data_used_n\" ,\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "    result_df = pd.concat([result_df,current_df])\n",
    "    letters = string.ascii_lowercase\n",
    "    suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "    print(suffix)\n",
    "    result_df[\"note\"] = \"add another layer at output neural network\"\n",
    "    result_df.to_csv(f\"2022-04-20_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd95728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d8f7445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for BATCH_SIZE in [1500]:\n",
    "# #     for LR in [1e-2]:\n",
    "# #         for N_HIDDEN_UNITS in [12]:\n",
    "# #             for NUM_LAYERS in [1]:\n",
    "# hyper_paras_list = [\n",
    "#     [80, 2500, 0.01, 20, 1]\n",
    "# ]\n",
    "\n",
    "# result_df = pd.DataFrame(columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\",\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "\n",
    "# for paras in hyper_paras_list:\n",
    "#     EPOCH,BATCH_SIZE,LR,N_HIDDEN_UNITS,NUM_LAYERS = paras\n",
    "    \n",
    "#     model  = LSTM_DNA(input_dim_n = N_FEATURES, hidden_units=N_HIDDEN_UNITS, num_layers=NUM_LAYERS)\n",
    "#     model = model.float()\n",
    "#     model.to(device)\n",
    "#     loss_function = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "    \n",
    "# #     train_dataset, val_dataset, test_dataset = train_dataset.to(device), val_dataset.to(device), test_dataset.to(device)\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     model_list = []\n",
    "#     train_loss_list = []\n",
    "#     val_loss_list = []\n",
    "\n",
    "#     print(f\"EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS = {EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS}\")\n",
    "    \n",
    "#     min_val_loss = 1e8\n",
    "#     lowest_epoch = 0\n",
    "#     PATH = f\"EPOCH{EPOCH}BATCH_SIZE{BATCH_SIZE}LR{LR}N_HIDDEN_UNITS{N_HIDDEN_UNITS}NUM_LAYERS{N_HIDDEN_UNITS}.pt\"\n",
    "#     print(PATH)\n",
    "    \n",
    "    \n",
    "#     for ix_epoch in range(EPOCH):\n",
    "#         print(f\"Epoch {ix_epoch} \")\n",
    "#         start = timeit.default_timer()\n",
    "#         train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "#         val_loss = eval_model(val_loader, model, loss_function)\n",
    "        \n",
    "#         train_loss_list.append(train_loss)\n",
    "#         val_loss_list.append(val_loss)\n",
    "        \n",
    "#         if val_loss < min_val_loss:\n",
    "#                 if os.path.exists(PATH):\n",
    "#                     os.remove(PATH)\n",
    "#                 torch.save(model.state_dict(), PATH)\n",
    "#                 min_val_loss = val_loss\n",
    "#                 lowest_epoch = ix_epoch\n",
    "        \n",
    "#         stop = timeit.default_timer()\n",
    "#         print(f'Epoch finished val_loss{val_loss}, Time Elapse: ', stop - start) \n",
    "    \n",
    "#     min_val_loss_id = np.argmin(val_loss_list)\n",
    "#     min_val_loss = val_loss_list[min_val_loss_id]\n",
    "    \n",
    "#     energy_predict = predict(test_loader, model)\n",
    "#     test_mse = get_avg_mse(test_df[\"energy\"].values,energy_predict)\n",
    "    \n",
    "#     print(f\"Total Epoch {EPOCH}\\n min_val_id {min_val_loss_id}, min_val_loss {min_val_loss}, test_mse {test_mse}\")\n",
    "\n",
    "#     model_result = [EPOCH, BATCH_SIZE, LR, N_HIDDEN_UNITS, NUM_LAYERS, DATA_USED_N,min_val_loss_id, min_val_loss, test_mse, train_loss_list, val_loss_list]\n",
    "\n",
    "#     current_df = pd.DataFrame([model_result], columns = [\"EPOCH\", \"BATCH_SIZE\", \"LR\", \"N_HIDDEN_UNITS\", \"NUM_LAYERS\",\"data_used_n\" ,\"min_val_loss_id\", \"min_val_loss\", \"test_mse\", \"train_loss_list\", \"val_loss_list\"])\n",
    "\n",
    "#     result_df = pd.concat([result_df,current_df])\n",
    "#     result_df.to_csv(\"2022-04-15_n.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9c2be8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# printing lowercase\n",
    "# letters = string.ascii_lowercase\n",
    "# suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "# suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a8247c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def x_transform(dna_str, pad=True):\n",
    "#     n_pad = MAX_LEN - len(dna_str) \n",
    "#     target = np.array(list(dna_str))\n",
    "#     onehot = np.array([DNA_ONE_HOT[letter] for letter in dna_str])\n",
    "# #     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "    \n",
    "#     if pad == True:\n",
    "#         return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     else:\n",
    "#         return onehot\n",
    "\n",
    "# str1 = \"CGATT\"\n",
    "# x_transform(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4283fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_A(str1):\n",
    "#     return str1.count(\"A\")\n",
    "\n",
    "# def count_G(str1):\n",
    "#     return str1.count(\"G\")\n",
    "\n",
    "# def count_C(str1):\n",
    "#     return str1.count(\"C\")\n",
    "\n",
    "# def count_T(str1):\n",
    "#     return str1.count(\"T\")    \n",
    "\n",
    "# def get_feature(str1):\n",
    "#     alln = len(str1)\n",
    "#     an = count_A(str1)\n",
    "#     tn = count_T(str1)\n",
    "#     cn = count_C(str1)\n",
    "#     gn = count_G(str1)\n",
    "#     rs = np.array([alln, an, tn, cn, gn]).reshape((1,5))\n",
    "#     return np.tile(rs, (alln,1))\n",
    "\n",
    "# def x_transform(dna_str, pad=PADDING):\n",
    "    \n",
    "#     fea = get_feature(dna_str)\n",
    "    \n",
    "#     n_pad = MAX_LEN - len(dna_str) \n",
    "#     target = np.array(list(dna_str))\n",
    "#     onehot = np.array([DNA_ONE_HOT[letter] for letter in dna_str])\n",
    "# #     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "#     print(onehot.shape, fea.shape)\n",
    "#     if pad == True:\n",
    "#         return np.pad(np.concatenate((onehot,fea), axis=1),((0,n_pad),(0,0)), mode='constant'), fea\n",
    "#     else:\n",
    "#         return np.concatenate((onehot,fea), axis=1) \n",
    "\n",
    "# # (np.array([DNA_ONE_HOT[letter]) for letter in dna_str]), get_feature(dna_str).reshape((1, 5))), axis=1)\n",
    "# #     one_hot_paded = np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "# #     if pad == True:\n",
    "# #         return np.pad(onehot,((0,n_pad),(0,0)), mode='constant')\n",
    "# #     else:\n",
    "# #         return onehot \n",
    "\n",
    "\n",
    "# n1 = get_feature(\"ATCG\")\n",
    "# n1.shape\n",
    "# x_transform(\"ATCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c240fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([1,2,3]).reshape((-1,3))\n",
    "# print(x.shape)\n",
    "# np.tile(x, (4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "676946f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_feature(\"CGATTTYA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac296ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc4a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d3330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ddb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd525602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792dcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
